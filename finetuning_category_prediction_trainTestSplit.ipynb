{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e24c8",
   "metadata": {},
   "source": [
    "Set the seeder to have as stable random operations as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2010727",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data = pd.read_csv('sequences_data.csv') # sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a021b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Vulnerability       Category  Length\n",
      "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9\n",
      "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8\n",
      "2  from django.http import httpresponse, httpresp...  open_redirect       9\n",
      "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175\n",
      "4                          update_query = self.up...  sql_injection      14\n"
     ]
    }
   ],
   "source": [
    "print(sequences_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7aee574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Frequencies:\n",
      " sql_injection            1424\n",
      "xsrf                      976\n",
      "command_injection         721\n",
      "path_disclosure           481\n",
      "open_redirect             442\n",
      "remote_code_execution     334\n",
      "xss                       145\n",
      "Name: Category, dtype: int64\n",
      "Total samples  4523\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = sequences_data['Category'].value_counts()\n",
    "print(\"Label Frequencies:\\n\", label_frequencies)\n",
    "print(\"Total samples \", len(sequences_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa5944e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sequences_data[\"Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6040f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 392\n"
     ]
    }
   ],
   "source": [
    "word_counts = sequences_data[\"Vulnerability\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5a91a",
   "metadata": {},
   "source": [
    "Pre-trained CodeBERT model - Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13eeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variation = \"microsoft/codebert-base-mlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "\n",
    "# Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "new_tokens = [\"strId$\", \"numId$\"]\n",
    "for new_token in new_tokens:\n",
    "    if new_token not in tokenizer.get_vocab().keys():\n",
    "        tokenizer.add_tokens(new_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac1d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user parameters\n",
    "n_epochs = 100\n",
    "batch_size = 8\n",
    "lr = 5e-05\n",
    "max_len = 512\n",
    "patience = 5\n",
    "#train_len = round(len(sequences_data) * 0.9)\n",
    "#sequences_data = sequences_data.iloc[0:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c653c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b22edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    learning_rate=lr, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadb2b2",
   "metadata": {},
   "source": [
    "Multi-class Classification: Categorization of all detected vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b444655",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = len(label_frequencies) # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d1281a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vulnerability</th>\n",
       "      <th>Category</th>\n",
       "      <th>Length</th>\n",
       "      <th>Category_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f\"str$id\"\"str$id\"\"str$id\"         ...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client.listentcp()    proxy = proxy(proxy_...</td>\n",
       "      <td>xsrf</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from django.http import httpresponse, httpresp...</td>\n",
       "      <td>open_redirect</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def write_preset(conn, queryin, descriptin):\\t...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>update_query = self.up...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Vulnerability       Category  Length  \\\n",
       "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9   \n",
       "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8   \n",
       "2  from django.http import httpresponse, httpresp...  open_redirect       9   \n",
       "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175   \n",
       "4                          update_query = self.up...  sql_injection      14   \n",
       "\n",
       "   Category_Index  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categories to numerical indexes\n",
    "category_numerical_indexes, unique_categories = sequences_data[\"Category\"].factorize()\n",
    "\n",
    "# Create a dictionary mapping each category to its numerical index\n",
    "category_to_index = {category: index for index, category in enumerate(unique_categories)}\n",
    "\n",
    "# Update the categories in the DataFrame with their numerical indexes\n",
    "sequences_data[\"Category_Index\"] = sequences_data[\"Category\"].map(category_to_index)\n",
    "sequences_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d56181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = tf.math.reduce_sum(tf.cast(input_ids != 1, tf.int32)).numpy()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "\n",
    "    #X['input_ids'] = np.delete(X['input_ids'], max_row, axis=0)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41097e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokenized data: 512\n",
      "Row with max length: 3\n",
      "Max tokenized length 512\n"
     ]
    }
   ],
   "source": [
    "X = sequences_data[\"Vulnerability\"].tolist()\n",
    "\n",
    "X = tokenizer(\n",
    "        text=X[0:],\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "max_len = getMaxLen(X)\n",
    "print(\"Max tokenized length\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e74011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_X(train_data_input, val_data_input, max_len):\n",
    "\n",
    "    X_train = tokenizer(\n",
    "        text=train_data_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    X_test = tokenizer(\n",
    "        text=val_data_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26aa30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset to train-test sets\n",
    "### split data into train and test (90% train, 10% test)\n",
    "shuffle_seeders = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(sequences_data[\"Vulnerability\"].tolist(), sequences_data[\"Category_Index\"].tolist(), stratify = sequences_data[\"Category_Index\"].tolist(), test_size=0.1, random_state=shuffle_seeder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03bc7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = tokenize_X(sequences_data[\"Vulnerability\"].tolist()[0:train_len], sequences_data[\"Vulnerability\"].tolist()[train_len:], max_len)\n",
    "# Y_train = sequences_data[\"Category_Index\"].tolist()[0:train_len]\n",
    "# Y_test = sequences_data[\"Category_Index\"].tolist()[train_len:]\n",
    "# Y_train = np.array(Y_train)\n",
    "# Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85a5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = tokenize_X(x_train, x_test, max_len)\n",
    "\n",
    "Y_train = np.array(y_train)\n",
    "Y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf610b0",
   "metadata": {},
   "source": [
    "Train - Test split, fit and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3470793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base-mlm and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.9481 - f1_metric: 0.6179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 386s 727ms/step - loss: 0.9481 - f1_metric: 0.6179 - val_loss: 0.6946 - val_f1_metric: 0.6880\n",
      "Epoch 2/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.5358 - f1_metric: 0.7191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 370s 726ms/step - loss: 0.5358 - f1_metric: 0.7191 - val_loss: 0.5580 - val_f1_metric: 0.7062\n",
      "Epoch 3/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.3977 - f1_metric: 0.7532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 372s 731ms/step - loss: 0.3977 - f1_metric: 0.7532 - val_loss: 0.5135 - val_f1_metric: 0.7297\n",
      "Epoch 4/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.3329 - f1_metric: 0.7740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 370s 726ms/step - loss: 0.3329 - f1_metric: 0.7740 - val_loss: 0.4978 - val_f1_metric: 0.7431\n",
      "Epoch 5/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.2827 - f1_metric: 0.7881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 368s 723ms/step - loss: 0.2827 - f1_metric: 0.7881 - val_loss: 0.4968 - val_f1_metric: 0.7487\n",
      "Epoch 6/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.2460 - f1_metric: 0.8058"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 370s 727ms/step - loss: 0.2460 - f1_metric: 0.8058 - val_loss: 0.4721 - val_f1_metric: 0.7623\n",
      "Epoch 7/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.2246 - f1_metric: 0.8084"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 369s 724ms/step - loss: 0.2246 - f1_metric: 0.8084 - val_loss: 0.4632 - val_f1_metric: 0.7743\n",
      "Epoch 8/100\n",
      "509/509 [==============================] - 349s 686ms/step - loss: 0.2003 - f1_metric: 0.8224 - val_loss: 0.4634 - val_f1_metric: 0.7799\n",
      "Epoch 9/100\n",
      "509/509 [==============================] - ETA: 0s - loss: 0.1887 - f1_metric: 0.8225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 371s 729ms/step - loss: 0.1887 - f1_metric: 0.8225 - val_loss: 0.4543 - val_f1_metric: 0.7811\n",
      "Epoch 10/100\n",
      "509/509 [==============================] - 347s 681ms/step - loss: 0.1717 - f1_metric: 0.8277 - val_loss: 0.4623 - val_f1_metric: 0.7852\n",
      "Epoch 11/100\n",
      "509/509 [==============================] - 347s 681ms/step - loss: 0.1634 - f1_metric: 0.8355 - val_loss: 0.4592 - val_f1_metric: 0.7842\n",
      "Epoch 12/100\n",
      "509/509 [==============================] - 346s 680ms/step - loss: 0.1483 - f1_metric: 0.8407 - val_loss: 0.4571 - val_f1_metric: 0.7931\n",
      "Epoch 13/100\n",
      "509/509 [==============================] - 345s 679ms/step - loss: 0.1424 - f1_metric: 0.8459 - val_loss: 0.4645 - val_f1_metric: 0.8024\n",
      "Epoch 14/100\n",
      "509/509 [==============================] - 344s 676ms/step - loss: 0.1342 - f1_metric: 0.8460 - val_loss: 0.4650 - val_f1_metric: 0.7902\n",
      "15/15 [==============================] - 12s 668ms/step\n",
      "Confusion Matrix:\n",
      " [[128   6   3   1   1   1   3]\n",
      " [  4  90   1   0   1   1   1]\n",
      " [  5   2  31   0   1   1   4]\n",
      " [  1   0   1  12   1   0   0]\n",
      " [  0   5   0   0  28   0   0]\n",
      " [  3   0   2   0   3  62   2]\n",
      " [  1   0   1   0   1   0  45]]\n",
      "Accuracy:87.42%\n",
      "Precision:86.33%\n",
      "Recall:85.22%\n",
      "F1 score:85.55%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       143\n",
      "           1       0.87      0.92      0.90        98\n",
      "           2       0.79      0.70      0.75        44\n",
      "           3       0.92      0.80      0.86        15\n",
      "           4       0.78      0.85      0.81        33\n",
      "           5       0.95      0.86      0.91        72\n",
      "           6       0.82      0.94      0.87        48\n",
      "\n",
      "    accuracy                           0.87       453\n",
      "   macro avg       0.86      0.85      0.86       453\n",
      "weighted avg       0.88      0.87      0.87       453\n",
      "\n",
      "Training is completed after 5071781\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[f1_metric]\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=patience)\n",
    "model_checkpoint = ModelCheckpoint('./checkpoints/best_weights', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x = {'input_ids':X_train['input_ids'], 'attention_mask':X_train['attention_mask']},\n",
    "    y = to_categorical(Y_train.astype(int)),\n",
    "    validation_data = ({'input_ids':X_test['input_ids'], 'attention_mask':X_test['attention_mask']},\n",
    "                        to_categorical(Y_test.astype(int))),\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "#model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_weights('./checkpoints/best_weights')\n",
    "\n",
    "predicted = model.predict({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']}).logits\n",
    "predictions = np.argmax(predicted, axis=1)\n",
    "\n",
    "targets = Y_test.astype(int)\n",
    "\n",
    "accuracy=accuracy_score(targets, predictions)\n",
    "precision=precision_score(targets, predictions, average='macro')\n",
    "recall=recall_score(targets, predictions, average='macro')\n",
    "f1=f1_score(targets, predictions, average='macro')\n",
    "conf_matrix = confusion_matrix(targets, predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "\n",
    "class_report = classification_report(targets, predictions)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77bcd802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8521590937407265"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f9990",
   "metadata": {},
   "source": [
    "85.43 87.20 86.95 85.87 85.65 87.42 86.75 86.98 89.85 88.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "135de81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.03999999999999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(85.43 + 87.20 + 86.95 + 85.87 + 85.65 +87.42 + 86.75 + 86.98 + 89.85 + 88.30)/10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
