{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader\n",
    "import io\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e24c8",
   "metadata": {},
   "source": [
    "Set the seeder to have as stable random operations as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a6448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_algorithms = [\"w2v\", \"bert\", \"codebert\"]\n",
    "embedding_algorithm = embedding_algorithms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f9b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"bow\", \"sequences\"]\n",
    "representation_form = representations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3acc7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary or multi-class\n",
    "multi = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a56a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2010727",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if representation_form == \"bow\":\n",
    "    data = pd.read_csv('bow_data.csv') # bow\n",
    "else:\n",
    "    data = pd.read_csv('sequences_data.csv') # sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abc9724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vulnerability</th>\n",
       "      <th>Category</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f\"str$id\"\"str$id\"\"str$id\"         ...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client.listentcp()    proxy = proxy(proxy_...</td>\n",
       "      <td>xsrf</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from django.http import httpresponse, httpresp...</td>\n",
       "      <td>open_redirect</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def write_preset(conn, queryin, descriptin):\\t...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>update_query = self.up...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Vulnerability       Category  Length\n",
       "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9\n",
       "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8\n",
       "2  from django.http import httpresponse, httpresp...  open_redirect       9\n",
       "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175\n",
       "4                          update_query = self.up...  sql_injection      14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5d5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if representation_form == \"bow\":\n",
    "    bow_size = 237 # number of columns that stand as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e46cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48846336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 392\n"
     ]
    }
   ],
   "source": [
    "if representation_form == \"sequences\":\n",
    "    word_counts = data[\"Vulnerability\"].apply(lambda x: len(x.split()))\n",
    "    max_length = word_counts.max()\n",
    "    print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bbacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Frequencies:\n",
      " sql_injection            1424\n",
      "xsrf                      976\n",
      "command_injection         721\n",
      "path_disclosure           481\n",
      "open_redirect             442\n",
      "remote_code_execution     334\n",
      "xss                       145\n",
      "Name: Category, dtype: int64\n",
      "Total samples  4523\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = data['Category'].value_counts()\n",
    "print(\"Label Frequencies:\\n\", label_frequencies)\n",
    "print(\"Total samples \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b777af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True:\n",
    "    n_categories = len(label_frequencies) # 7\n",
    "    n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d18f",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acbd1",
   "metadata": {},
   "source": [
    "Word2Vec - load pre-trained word2vec embeddings - NL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0d6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaskaloup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if representation_form == \"sequences\":\n",
    "\n",
    "    # Download the Punkt tokenizer models if not already downloaded\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    tokenized_list = [word_tokenize(sentence) for sentence in data[\"Vulnerability\"].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9edbb",
   "metadata": {},
   "source": [
    "BERT - load pre-trained bert embeddings - NL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b99ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"bert\" and representation_form == \"sequences\": \n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)\n",
    "    \n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c734b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"bert\" and representation_form == \"sequences\":\n",
    "    sentences = data[\"Vulnerability\"].tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "    \n",
    "    lines_pad = []\n",
    "    for seq in sequences:\n",
    "        lines_pad.append(seq[0])\n",
    "    \n",
    "    lines_pad = pad_sequences(lines_pad, padding = 'post', maxlen = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41a30e",
   "metadata": {},
   "source": [
    "CodeBERT - load pre-trained codebert embeddings - PL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be0cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"codebert\" and representation_form == \"sequences\": \n",
    "    model_variation = \"microsoft/codebert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation)\n",
    "    \n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f596d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02135a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"codebert\" and representation_form == \"sequences\": \n",
    "\n",
    "    sentences = data[\"Vulnerability\"].tolist()\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]\n",
    "    \n",
    "    lines_pad = padSequences(sequences, 512)\n",
    "    lines_pad = [arr.tolist() for arr in lines_pad]\n",
    "    lines_pad = np.array(lines_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ce97",
   "metadata": {},
   "source": [
    "RNN model, LSTM specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99be2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMLP(n_inputs, n_outputs):\n",
    "    nIn = n_inputs\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(256, input_dim=nIn, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)))#hidden\n",
    "    model.add(Dense(256, input_dim=nIn))#hidden\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))#hidden\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256))#hidden\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs)) #binary classification\n",
    "    model.add(Activation('softmax'))#Output # sigmoid # softmax\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001) #, epsilon=1e-8, decay=1e-6\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer) # 'binary_crossentropy' # \"sparse_categorical_crossentropy\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5472fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ffe75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(words, w2v_vectors):\n",
    "    words_vecs = [w2v_vectors[word] for word in words if word in w2v_vectors]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(300)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a86bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab4fc2",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3dea",
   "metadata": {},
   "source": [
    "Multi-class Classification: Categorization of all detected vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab8dd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True:\n",
    "    \n",
    "    # Convert categories to numerical indexes\n",
    "    category_numerical_indexes, unique_categories = data[\"Category\"].factorize()\n",
    "\n",
    "    # Create a dictionary mapping each category to its numerical index\n",
    "    category_to_index = {category: index for index, category in enumerate(unique_categories)}\n",
    "\n",
    "    # Update the categories in the DataFrame with their numerical indexes\n",
    "    data[\"Category_Index\"] = data[\"Category\"].map(category_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28251b1",
   "metadata": {},
   "source": [
    "Use DL models and sequences of tokens code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "881e21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDLCrossVal_multi(X, y, w2v_vectors, max_len, seed):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train = np.array([vectorize(sentence, w2v_vectors) for sentence in X_train])\n",
    "        X_test = np.array([vectorize(sentence, w2v_vectors) for sentence in X_test])\n",
    "        \n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        myModel = buildMLP(X_train.shape[1], n_categories)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = np.argmax(predScores, axis=1)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='macro')\n",
    "        recall=recall_score(Y_test, predictions, average='macro')\n",
    "        f1=f1_score(Y_test, predictions, average='macro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd4ae015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "fold number=  1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 4s 5ms/step - loss: 1.6965 - val_loss: 1.5375\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53755, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.4999 - val_loss: 1.3941\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53755 to 1.39412, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3889 - val_loss: 1.3297\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.39412 to 1.32969, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3150 - val_loss: 1.2733\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32969 to 1.27332, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2554 - val_loss: 1.2168\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27332 to 1.21684, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1985 - val_loss: 1.1975\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.21684 to 1.19745, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1539 - val_loss: 1.1671\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.19745 to 1.16714, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1198 - val_loss: 1.1318\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.16714 to 1.13180, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0811 - val_loss: 1.1238\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.13180 to 1.12384, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0553 - val_loss: 1.1232\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.12384 to 1.12318, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0310 - val_loss: 1.0887\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.12318 to 1.08871, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9893 - val_loss: 1.0885\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.08871 to 1.08849, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9652 - val_loss: 1.0817\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.08849 to 1.08168, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9418 - val_loss: 1.0533\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.08168 to 1.05329, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9203 - val_loss: 1.0644\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.05329\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9070 - val_loss: 1.0551\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05329\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8835 - val_loss: 1.0538\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05329\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8748 - val_loss: 1.0551\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05329\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8564 - val_loss: 1.0639\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05329\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8474 - val_loss: 1.0647\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.05329\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8211 - val_loss: 1.0684\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.05329\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8080 - val_loss: 1.0635\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.05329\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8152 - val_loss: 1.0557\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.05329\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7907 - val_loss: 1.0358\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.05329 to 1.03581, saving model to best_model.h5\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7753 - val_loss: 1.0708\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.03581\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7862 - val_loss: 1.0575\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.03581\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7663 - val_loss: 1.1189\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.03581\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7596 - val_loss: 1.0877\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.03581\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7512 - val_loss: 1.0875\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.03581\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7623 - val_loss: 1.0470\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.03581\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7314 - val_loss: 1.0878\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.03581\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7237 - val_loss: 1.0790\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.03581\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7218 - val_loss: 1.0865\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.03581\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7274 - val_loss: 1.0871\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.03581\n",
      "Epoch 00034: early stopping\n",
      "[[98 29  5  0  4  5  2]\n",
      " [ 3 80  6  0  0  7  1]\n",
      " [ 4 15 21  0  1  1  3]\n",
      " [ 3  6  3  2  0  0  0]\n",
      " [ 0 12  6  0 15  1  0]\n",
      " [ 8  9  2  0  2 51  0]\n",
      " [ 3 12  4  0  1  2 26]]\n",
      "Accuracy:64.68%\n",
      "Precision:64.68%\n",
      "Recall:64.68%\n",
      "F1 score:64.68%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.69      0.75       143\n",
      "           1       0.49      0.82      0.62        97\n",
      "           2       0.45      0.47      0.46        45\n",
      "           3       1.00      0.14      0.25        14\n",
      "           4       0.65      0.44      0.53        34\n",
      "           5       0.76      0.71      0.73        72\n",
      "           6       0.81      0.54      0.65        48\n",
      "\n",
      "    accuracy                           0.65       453\n",
      "   macro avg       0.71      0.54      0.57       453\n",
      "weighted avg       0.70      0.65      0.65       453\n",
      "\n",
      "fold number=  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 1.7092 - val_loss: 1.5527\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55272, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5223 - val_loss: 1.4106\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55272 to 1.41057, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4066 - val_loss: 1.3200\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41057 to 1.32004, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3362 - val_loss: 1.2762\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32004 to 1.27622, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2674 - val_loss: 1.2331\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27622 to 1.23311, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2047 - val_loss: 1.2040\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.23311 to 1.20397, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1608 - val_loss: 1.1758\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.20397 to 1.17576, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1100 - val_loss: 1.1477\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.17576 to 1.14765, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0836 - val_loss: 1.1241\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.14765 to 1.12408, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0526 - val_loss: 1.1218\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.12408 to 1.12176, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0243 - val_loss: 1.0902\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.12176 to 1.09022, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0076 - val_loss: 1.0876\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.09022 to 1.08757, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9681 - val_loss: 1.0719\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.08757 to 1.07190, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9427 - val_loss: 1.0692\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.07190 to 1.06916, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9155 - val_loss: 1.0728\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.06916\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9005 - val_loss: 1.0571\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.06916 to 1.05711, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8888 - val_loss: 1.0740\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05711\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8684 - val_loss: 1.0768\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05711\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8508 - val_loss: 1.0868\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05711\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8374 - val_loss: 1.0633\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.05711\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8247 - val_loss: 1.0346\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.05711 to 1.03460, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8119 - val_loss: 1.0654\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.03460\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8120 - val_loss: 1.0686\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.03460\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7991 - val_loss: 1.0386\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.03460\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7843 - val_loss: 1.0598\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.03460\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7692 - val_loss: 1.0729\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.03460\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7740 - val_loss: 1.0649\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.03460\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7585 - val_loss: 1.0925\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.03460\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7745 - val_loss: 1.0376\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.03460\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7352 - val_loss: 1.0328\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.03460 to 1.03277, saving model to best_model.h5\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7437 - val_loss: 1.0769\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.03277\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7270 - val_loss: 1.0352\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.03277\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7350 - val_loss: 1.0454\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.03277\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.719 - 0s 5ms/step - loss: 0.7169 - val_loss: 1.0688\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.03277\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7124 - val_loss: 1.0459\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.03277\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7077 - val_loss: 1.0496\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.03277\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.6925 - val_loss: 1.0327\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.03277 to 1.03266, saving model to best_model.h5\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7093 - val_loss: 1.0796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss did not improve from 1.03266\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 1.0783\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.03266\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 1.0615\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.03266\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.6874 - val_loss: 1.1116\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.03266\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.6956 - val_loss: 1.0454\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.03266\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.6821 - val_loss: 1.0456\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.03266\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.6706 - val_loss: 1.0951\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.03266\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.6633 - val_loss: 1.0340\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.03266\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.6780 - val_loss: 1.0462\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.03266\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.6635 - val_loss: 1.0711\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.03266\n",
      "Epoch 00047: early stopping\n",
      "[[97 35  3  0  2  6  0]\n",
      " [ 1 85  3  0  0  7  1]\n",
      " [ 2 20 15  0  0  8  0]\n",
      " [ 0  0  0 11  0  3  0]\n",
      " [ 1 13  0  0 18  1  0]\n",
      " [ 6 11  1  1  0 49  5]\n",
      " [ 2 15  0  0  0  5 26]]\n",
      "Accuracy:66.45%\n",
      "Precision:66.45%\n",
      "Recall:66.45%\n",
      "F1 score:66.45%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.68      0.77       143\n",
      "           1       0.47      0.88      0.62        97\n",
      "           2       0.68      0.33      0.45        45\n",
      "           3       0.92      0.79      0.85        14\n",
      "           4       0.90      0.55      0.68        33\n",
      "           5       0.62      0.67      0.64        73\n",
      "           6       0.81      0.54      0.65        48\n",
      "\n",
      "    accuracy                           0.66       453\n",
      "   macro avg       0.76      0.63      0.66       453\n",
      "weighted avg       0.73      0.66      0.67       453\n",
      "\n",
      "fold number=  3\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 5ms/step - loss: 1.6921 - val_loss: 1.6119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61191, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4947 - val_loss: 1.4713\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61191 to 1.47133, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3808 - val_loss: 1.4006\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.47133 to 1.40057, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3070 - val_loss: 1.3704\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.40057 to 1.37039, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2358 - val_loss: 1.3520\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.37039 to 1.35203, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1975 - val_loss: 1.3079\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.35203 to 1.30787, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1442 - val_loss: 1.2727\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.30787 to 1.27275, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1054 - val_loss: 1.2593\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.27275 to 1.25928, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0722 - val_loss: 1.2441\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.25928 to 1.24414, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0349 - val_loss: 1.2618\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.24414\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0087 - val_loss: 1.2443\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.24414\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9788 - val_loss: 1.1834\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.24414 to 1.18335, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9518 - val_loss: 1.1683\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.18335 to 1.16828, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9335 - val_loss: 1.2011\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.16828\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9165 - val_loss: 1.1629\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.16828 to 1.16290, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8949 - val_loss: 1.1613\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.16290 to 1.16131, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8853 - val_loss: 1.1616\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.16131\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8591 - val_loss: 1.1955\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.16131\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8425 - val_loss: 1.1911\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.16131\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8320 - val_loss: 1.1858\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.16131\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.8100 - val_loss: 1.2247\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.16131\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8058 - val_loss: 1.2149\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.16131\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7998 - val_loss: 1.2106\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.16131\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7823 - val_loss: 1.1977\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.16131\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7747 - val_loss: 1.1875\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.16131\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7589 - val_loss: 1.2242\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.16131\n",
      "Epoch 00026: early stopping\n",
      "[[100  21   4   0   4  12   2]\n",
      " [  3  84   1   0   5   4   0]\n",
      " [  7  17  12   1   4   3   0]\n",
      " [  3   4   1   4   2   1   0]\n",
      " [  3   9   0   0  19   1   1]\n",
      " [  7  10   4   0   1  47   3]\n",
      " [ 10  11   3   1   3   6  15]]\n",
      "Accuracy:62.03%\n",
      "Precision:62.03%\n",
      "Recall:62.03%\n",
      "F1 score:62.03%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.70      0.72       143\n",
      "           1       0.54      0.87      0.66        97\n",
      "           2       0.48      0.27      0.35        44\n",
      "           3       0.67      0.27      0.38        15\n",
      "           4       0.50      0.58      0.54        33\n",
      "           5       0.64      0.65      0.64        72\n",
      "           6       0.71      0.31      0.43        49\n",
      "\n",
      "    accuracy                           0.62       453\n",
      "   macro avg       0.61      0.52      0.53       453\n",
      "weighted avg       0.64      0.62      0.61       453\n",
      "\n",
      "fold number=  4\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 5ms/step - loss: 1.7089 - val_loss: 1.5311\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53113, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5159 - val_loss: 1.4084\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53113 to 1.40841, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3830 - val_loss: 1.3335\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.40841 to 1.33346, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3009 - val_loss: 1.3103\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.33346 to 1.31028, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2434 - val_loss: 1.2669\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31028 to 1.26690, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1947 - val_loss: 1.2406\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26690 to 1.24064, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1579 - val_loss: 1.2267\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.24064 to 1.22671, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1179 - val_loss: 1.1939\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.22671 to 1.19394, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0747 - val_loss: 1.1944\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.19394\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0451 - val_loss: 1.1664\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.19394 to 1.16635, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0225 - val_loss: 1.1542\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.16635 to 1.15420, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0032 - val_loss: 1.1547\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.15420\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9631 - val_loss: 1.1698\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.15420\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9329 - val_loss: 1.1540\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.15420 to 1.15404, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9103 - val_loss: 1.1644\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.15404\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9037 - val_loss: 1.1364\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.15404 to 1.13645, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8866 - val_loss: 1.1421\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.13645\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8682 - val_loss: 1.1711\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.13645\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8653 - val_loss: 1.1498\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.13645\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8401 - val_loss: 1.1479\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.13645\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8140 - val_loss: 1.1586\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.13645\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8040 - val_loss: 1.2114\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.13645\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8114 - val_loss: 1.1708\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.13645\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7968 - val_loss: 1.1995\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.13645\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7953 - val_loss: 1.2237\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.13645\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7758 - val_loss: 1.2388\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.13645\n",
      "Epoch 00026: early stopping\n",
      "[[99 27  1  1  2 12  1]\n",
      " [ 5 84  0  0  0  7  1]\n",
      " [ 9 14 14  0  0  3  4]\n",
      " [ 6  3  0  2  0  4  0]\n",
      " [ 3  9  1  1 16  2  1]\n",
      " [13  8  2  0  1 45  3]\n",
      " [ 6 13  1  2  2  8 16]]\n",
      "Accuracy:61.06%\n",
      "Precision:61.06%\n",
      "Recall:61.06%\n",
      "F1 score:61.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.70       143\n",
      "           1       0.53      0.87      0.66        97\n",
      "           2       0.74      0.32      0.44        44\n",
      "           3       0.33      0.13      0.19        15\n",
      "           4       0.76      0.48      0.59        33\n",
      "           5       0.56      0.62      0.59        72\n",
      "           6       0.62      0.33      0.43        48\n",
      "\n",
      "    accuracy                           0.61       452\n",
      "   macro avg       0.61      0.49      0.51       452\n",
      "weighted avg       0.63      0.61      0.59       452\n",
      "\n",
      "fold number=  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 1.7075 - val_loss: 1.5925\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59249, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5038 - val_loss: 1.4480\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59249 to 1.44798, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4011 - val_loss: 1.3687\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44798 to 1.36872, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3171 - val_loss: 1.3134\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36872 to 1.31343, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2667 - val_loss: 1.2745\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31343 to 1.27452, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2207 - val_loss: 1.2729\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.27452 to 1.27289, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1736 - val_loss: 1.2190\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.27289 to 1.21901, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1397 - val_loss: 1.1946\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.21901 to 1.19463, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0953 - val_loss: 1.1928\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.19463 to 1.19280, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0715 - val_loss: 1.1562\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.19280 to 1.15616, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0344 - val_loss: 1.1581\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.15616\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0108 - val_loss: 1.1384\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.15616 to 1.13843, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9839 - val_loss: 1.1490\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.13843\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9603 - val_loss: 1.1148\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.13843 to 1.11482, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9306 - val_loss: 1.1318\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.11482\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9234 - val_loss: 1.1280\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.11482\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8991 - val_loss: 1.0982\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.11482 to 1.09820, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8781 - val_loss: 1.0979\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.09820 to 1.09788, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8592 - val_loss: 1.1139\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.09788\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8480 - val_loss: 1.0909\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.09788 to 1.09090, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8285 - val_loss: 1.1166\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.09090\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8207 - val_loss: 1.0943\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.09090\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8139 - val_loss: 1.0922\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.09090\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7974 - val_loss: 1.1404\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.09090\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7890 - val_loss: 1.1237\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.09090\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7780 - val_loss: 1.1029\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.09090\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7712 - val_loss: 1.0959\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.09090\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7665 - val_loss: 1.1234\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.09090\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 1.1311\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.09090\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7434 - val_loss: 1.1544\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.09090\n",
      "Epoch 00030: early stopping\n",
      "[[107  22   2   0   1   9   1]\n",
      " [  5  84   3   0   0   3   3]\n",
      " [  7  16  14   0   0   3   4]\n",
      " [  6   2   0   1   2   3   1]\n",
      " [  5   8   0   0  17   2   1]\n",
      " [ 11   7   1   0   0  52   1]\n",
      " [  9  10   0   0   1   9  19]]\n",
      "Accuracy:65.04%\n",
      "Precision:65.04%\n",
      "Recall:65.04%\n",
      "F1 score:65.04%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73       142\n",
      "           1       0.56      0.86      0.68        98\n",
      "           2       0.70      0.32      0.44        44\n",
      "           3       1.00      0.07      0.12        15\n",
      "           4       0.81      0.52      0.63        33\n",
      "           5       0.64      0.72      0.68        72\n",
      "           6       0.63      0.40      0.49        48\n",
      "\n",
      "    accuracy                           0.65       452\n",
      "   macro avg       0.72      0.52      0.54       452\n",
      "weighted avg       0.68      0.65      0.63       452\n",
      "\n",
      "fold number=  6\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 6ms/step - loss: 1.7050 - val_loss: 1.5626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56260, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5153 - val_loss: 1.4673\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56260 to 1.46726, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4010 - val_loss: 1.3776\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46726 to 1.37764, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3099 - val_loss: 1.3303\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37764 to 1.33032, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2565 - val_loss: 1.2962\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33032 to 1.29621, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1985 - val_loss: 1.2604\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29621 to 1.26039, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1558 - val_loss: 1.2410\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26039 to 1.24104, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1094 - val_loss: 1.2353\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.24104 to 1.23526, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0656 - val_loss: 1.2030\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.23526 to 1.20297, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0438 - val_loss: 1.2058\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.20297\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0037 - val_loss: 1.2101\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.20297\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9777 - val_loss: 1.2070\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.20297\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9604 - val_loss: 1.2278\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.20297\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9403 - val_loss: 1.1780\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.20297 to 1.17804, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9199 - val_loss: 1.2233\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.17804\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8877 - val_loss: 1.2007\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.17804\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8701 - val_loss: 1.2213\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.17804\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8694 - val_loss: 1.1960\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.17804\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8381 - val_loss: 1.1966\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.17804\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8336 - val_loss: 1.2234\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.17804\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8185 - val_loss: 1.2027\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.17804\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8034 - val_loss: 1.2146\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.17804\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8070 - val_loss: 1.2360\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.17804\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7836 - val_loss: 1.2084\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.17804\n",
      "Epoch 00024: early stopping\n",
      "[[97 23  7  1  2  9  3]\n",
      " [ 7 78  2  0  3  4  4]\n",
      " [ 6 18  7  1  4  6  2]\n",
      " [ 4  5  0  4  0  2  0]\n",
      " [ 2 11  2  0 11  4  3]\n",
      " [12 13  1  0  0 45  1]\n",
      " [ 5  8  3  1  0  4 27]]\n",
      "Accuracy:59.51%\n",
      "Precision:59.51%\n",
      "Recall:59.51%\n",
      "F1 score:59.51%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.71       142\n",
      "           1       0.50      0.80      0.61        98\n",
      "           2       0.32      0.16      0.21        44\n",
      "           3       0.57      0.27      0.36        15\n",
      "           4       0.55      0.33      0.42        33\n",
      "           5       0.61      0.62      0.62        72\n",
      "           6       0.68      0.56      0.61        48\n",
      "\n",
      "    accuracy                           0.60       452\n",
      "   macro avg       0.56      0.49      0.51       452\n",
      "weighted avg       0.60      0.60      0.58       452\n",
      "\n",
      "fold number=  7\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 1.6958 - val_loss: 1.6040\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60396, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5120 - val_loss: 1.4878\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60396 to 1.48779, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3976 - val_loss: 1.4286\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.48779 to 1.42861, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3107 - val_loss: 1.3872\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42861 to 1.38716, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2646 - val_loss: 1.3345\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.38716 to 1.33445, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2001 - val_loss: 1.2926\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.33445 to 1.29260, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1583 - val_loss: 1.2762\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.29260 to 1.27624, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1315 - val_loss: 1.2378\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.27624 to 1.23778, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0936 - val_loss: 1.2169\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.23778 to 1.21687, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0618 - val_loss: 1.2290\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.21687\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0361 - val_loss: 1.2078\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.21687 to 1.20784, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0044 - val_loss: 1.1740\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.20784 to 1.17397, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9800 - val_loss: 1.1558\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.17397 to 1.15581, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9567 - val_loss: 1.1604\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.15581\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9327 - val_loss: 1.1932\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.15581\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9099 - val_loss: 1.1429\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.15581 to 1.14288, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9027 - val_loss: 1.1177\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.14288 to 1.11766, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8731 - val_loss: 1.1258\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.11766\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8685 - val_loss: 1.1255\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.11766\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8580 - val_loss: 1.1185\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.11766\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8347 - val_loss: 1.1262\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.11766\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8230 - val_loss: 1.1299\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.11766\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8203 - val_loss: 1.1328\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.11766\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8023 - val_loss: 1.1318\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.11766\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7926 - val_loss: 1.0961\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.11766 to 1.09611, saving model to best_model.h5\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7858 - val_loss: 1.1314\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.09611\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7802 - val_loss: 1.1346\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.09611\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7815 - val_loss: 1.1478\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.09611\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7604 - val_loss: 1.1497\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.09611\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7557 - val_loss: 1.1402\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.09611\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7526 - val_loss: 1.1730\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.09611\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.7312 - val_loss: 1.2101\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.09611\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7453 - val_loss: 1.1745\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.09611\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7100 - val_loss: 1.1943\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.09611\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7255 - val_loss: 1.1798\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.09611\n",
      "Epoch 00035: early stopping\n",
      "[[112  16   1   0   3   7   3]\n",
      " [  6  76   3   1   2   9   1]\n",
      " [  4  14  20   1   1   4   0]\n",
      " [  2   4   0   7   0   1   1]\n",
      " [  4   7   0   0  15   4   3]\n",
      " [ 12   3   1   0   1  55   0]\n",
      " [  3  14   1   1   1   5  23]]\n",
      "Accuracy:68.14%\n",
      "Precision:68.14%\n",
      "Recall:68.14%\n",
      "F1 score:68.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79       142\n",
      "           1       0.57      0.78      0.66        98\n",
      "           2       0.77      0.45      0.57        44\n",
      "           3       0.70      0.47      0.56        15\n",
      "           4       0.65      0.45      0.54        33\n",
      "           5       0.65      0.76      0.70        72\n",
      "           6       0.74      0.48      0.58        48\n",
      "\n",
      "    accuracy                           0.68       452\n",
      "   macro avg       0.69      0.60      0.63       452\n",
      "weighted avg       0.70      0.68      0.68       452\n",
      "\n",
      "fold number=  8\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 5ms/step - loss: 1.6988 - val_loss: 1.5952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59517, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4994 - val_loss: 1.4407\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59517 to 1.44074, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3876 - val_loss: 1.4157\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44074 to 1.41572, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3085 - val_loss: 1.3744\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.41572 to 1.37442, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2473 - val_loss: 1.3414\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.37442 to 1.34143, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1958 - val_loss: 1.3195\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.34143 to 1.31946, saving model to best_model.h5\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1456 - val_loss: 1.2991\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.31946 to 1.29908, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1134 - val_loss: 1.2778\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.29908 to 1.27778, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0756 - val_loss: 1.2669\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.27778 to 1.26692, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0449 - val_loss: 1.2549\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.26692 to 1.25485, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0136 - val_loss: 1.2679\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.25485\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.9870 - val_loss: 1.2632\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.25485\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9567 - val_loss: 1.2513\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.25485 to 1.25132, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9358 - val_loss: 1.2580\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.25132\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9248 - val_loss: 1.2520\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.25132\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8940 - val_loss: 1.2524\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.25132\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8926 - val_loss: 1.2492\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.25132 to 1.24922, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8689 - val_loss: 1.2297\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.24922 to 1.22968, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8471 - val_loss: 1.2524\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.22968\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8433 - val_loss: 1.2469\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.22968\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8275 - val_loss: 1.2587\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.22968\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8312 - val_loss: 1.2790\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.22968\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.8013 - val_loss: 1.2893\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.22968\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8021 - val_loss: 1.2344\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.22968\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7793 - val_loss: 1.2712\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.22968\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7676 - val_loss: 1.2893\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.22968\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7631 - val_loss: 1.3134\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.22968\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7459 - val_loss: 1.3276\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.22968\n",
      "Epoch 00028: early stopping\n",
      "[[93 23  5  0  6 11  4]\n",
      " [ 4 80  4  0  0  9  1]\n",
      " [ 5 15 18  0  4  1  1]\n",
      " [ 5  2  3  2  0  2  0]\n",
      " [ 1  5  3  0 19  4  2]\n",
      " [ 5 16  3  0  3 43  2]\n",
      " [ 4  9  2  2  4  7 20]]\n",
      "Accuracy:60.84%\n",
      "Precision:60.84%\n",
      "Recall:60.84%\n",
      "F1 score:60.84%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.72       142\n",
      "           1       0.53      0.82      0.65        98\n",
      "           2       0.47      0.41      0.44        44\n",
      "           3       0.50      0.14      0.22        14\n",
      "           4       0.53      0.56      0.54        34\n",
      "           5       0.56      0.60      0.58        72\n",
      "           6       0.67      0.42      0.51        48\n",
      "\n",
      "    accuracy                           0.61       452\n",
      "   macro avg       0.58      0.51      0.52       452\n",
      "weighted avg       0.63      0.61      0.60       452\n",
      "\n",
      "fold number=  9\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 1.7042 - val_loss: 1.5555\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55546, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5039 - val_loss: 1.4025\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55546 to 1.40246, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3915 - val_loss: 1.3274\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.40246 to 1.32740, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3194 - val_loss: 1.2912\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32740 to 1.29123, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2554 - val_loss: 1.2626\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.29123 to 1.26262, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2044 - val_loss: 1.2370\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26262 to 1.23696, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1539 - val_loss: 1.2239\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.23696 to 1.22389, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1227 - val_loss: 1.1953\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.22389 to 1.19526, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0903 - val_loss: 1.1995\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.19526\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0440 - val_loss: 1.1757\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.19526 to 1.17567, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0097 - val_loss: 1.1887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 1.17567\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9953 - val_loss: 1.1572\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.17567 to 1.15719, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9558 - val_loss: 1.1510\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.15719 to 1.15103, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9429 - val_loss: 1.1701\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.15103\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9154 - val_loss: 1.1976\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.15103\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9065 - val_loss: 1.1697\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.15103\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8817 - val_loss: 1.1562\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.15103\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8776 - val_loss: 1.1679\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.15103\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8460 - val_loss: 1.1869\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.15103\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8403 - val_loss: 1.1958\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.15103\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8323 - val_loss: 1.2163\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.15103\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8187 - val_loss: 1.2299\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.15103\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7867 - val_loss: 1.2347\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.15103\n",
      "Epoch 00023: early stopping\n",
      "[[98 24  0  0  4 11  5]\n",
      " [ 1 89  2  0  1  5  0]\n",
      " [ 5 20  6  0  5  6  2]\n",
      " [ 1  5  1  3  0  4  0]\n",
      " [ 6  9  1  0 18  0  0]\n",
      " [ 9 10  0  0  0 51  2]\n",
      " [ 5 11  1  0  7  4 20]]\n",
      "Accuracy:63.05%\n",
      "Precision:63.05%\n",
      "Recall:63.05%\n",
      "F1 score:63.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.73       142\n",
      "           1       0.53      0.91      0.67        98\n",
      "           2       0.55      0.14      0.22        44\n",
      "           3       1.00      0.21      0.35        14\n",
      "           4       0.51      0.53      0.52        34\n",
      "           5       0.63      0.71      0.67        72\n",
      "           6       0.69      0.42      0.52        48\n",
      "\n",
      "    accuracy                           0.63       452\n",
      "   macro avg       0.67      0.51      0.53       452\n",
      "weighted avg       0.66      0.63      0.61       452\n",
      "\n",
      "fold number=  10\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 144,775\n",
      "Trainable params: 144,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 1.6991 - val_loss: 1.5245\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.52445, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5056 - val_loss: 1.4075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.52445 to 1.40754, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3967 - val_loss: 1.3200\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.40754 to 1.31996, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3240 - val_loss: 1.2737\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.31996 to 1.27372, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2647 - val_loss: 1.2393\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27372 to 1.23927, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2229 - val_loss: 1.2122\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.23927 to 1.21222, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1623 - val_loss: 1.1726\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.21222 to 1.17260, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1225 - val_loss: 1.1342\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.17260 to 1.13418, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1036 - val_loss: 1.1228\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.13418 to 1.12277, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0606 - val_loss: 1.0913\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.12277 to 1.09135, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0300 - val_loss: 1.0928\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.09135\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0042 - val_loss: 1.0507\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.09135 to 1.05068, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9691 - val_loss: 1.0304\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.05068 to 1.03043, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9477 - val_loss: 1.0476\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.03043\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9346 - val_loss: 1.0535\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.03043\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9027 - val_loss: 1.0238\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.03043 to 1.02381, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8954 - val_loss: 1.0144\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.02381 to 1.01440, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8816 - val_loss: 0.9881\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.01440 to 0.98810, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8601 - val_loss: 1.0055\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.98810\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8482 - val_loss: 1.0324\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.98810\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8391 - val_loss: 1.0313\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.98810\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8190 - val_loss: 1.0330\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.98810\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.8101 - val_loss: 1.0187\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.98810\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7968 - val_loss: 0.9970\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.98810\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7923 - val_loss: 1.0018\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.98810\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7735 - val_loss: 0.9833\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.98810 to 0.98333, saving model to best_model.h5\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7603 - val_loss: 0.9991\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.98333\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7687 - val_loss: 1.0040\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.98333\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7494 - val_loss: 1.0554\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.98333\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7424 - val_loss: 1.0140\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.98333\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7464 - val_loss: 1.0433\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.98333\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7392 - val_loss: 1.0181\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.98333\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7269 - val_loss: 1.0311\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.98333\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7315 - val_loss: 1.0172\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.98333\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7147 - val_loss: 1.0775\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.98333\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 1.0814\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.98333\n",
      "Epoch 00036: early stopping\n",
      "[[114   6   8   0   2   7   5]\n",
      " [ 11  74   3   1   1   7   1]\n",
      " [  6  15  19   1   0   2   1]\n",
      " [  5   2   0   4   1   1   1]\n",
      " [  5   7   3   0  14   1   4]\n",
      " [ 11   6   1   0   2  48   4]\n",
      " [  5   7   2   0   2   4  28]]\n",
      "Accuracy:66.59%\n",
      "Precision:66.59%\n",
      "Recall:66.59%\n",
      "F1 score:66.59%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76       142\n",
      "           1       0.63      0.76      0.69        98\n",
      "           2       0.53      0.43      0.48        44\n",
      "           3       0.67      0.29      0.40        14\n",
      "           4       0.64      0.41      0.50        34\n",
      "           5       0.69      0.67      0.68        72\n",
      "           6       0.64      0.58      0.61        48\n",
      "\n",
      "    accuracy                           0.67       452\n",
      "   macro avg       0.64      0.56      0.59       452\n",
      "weighted avg       0.66      0.67      0.66       452\n",
      "\n",
      "Cross Validation is completed after 110810\n",
      "accuracy: 63.74% (2.72%)\n",
      "precision: 63.74% (2.72%)\n",
      "recall: 63.74% (2.72%)\n",
      "f1: 63.74% (2.72%)\n"
     ]
    }
   ],
   "source": [
    "if multi == True and representation_form == \"sequences\" and train_w2v == False:\n",
    "    runDLCrossVal_multi(np.array(tokenized_list), data[\"Category_Index\"].values, w2v_vectors, 512, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b9aa3",
   "metadata": {},
   "source": [
    "One more option for training your own word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55864532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWord2Vec(tokenized_list, max_length, dim):\n",
    "    # this should be executed only in the training set during cross-validation\n",
    "    w2v_model = Word2Vec(sentences=tokenized_list, vector_size=dim, window=5, min_count=1, workers=4, epochs=4)\n",
    "    #w2v_model.save(\"python_word2vec.model\")\n",
    "    #w2v_model = Word2Vec.load(\"python_word2vec.model\")\n",
    "\n",
    "    #     fileEmb = 'w2v_embeddingsIters.txt'\n",
    "    #     w2v_vectors.save_word2vec_format(fileEmb, binary=False)\n",
    "\n",
    "    #     embeddings_index = {}\n",
    "    #     f = open(os.path.join('', fileEmb), encoding=\"utf-8\")\n",
    "    #     for line in f:    \n",
    "    #         values = line.split()\n",
    "    #         word = values[0]\n",
    "    #         coefs = np.asarray(values[1:])\n",
    "    #         embeddings_index[word] = coefs   \n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "\n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'w2v_new_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "\n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "\n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "\n",
    "\n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = w2v_model.wv[word] if word in w2v_model.wv else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return lines_pad, embedding_matrix, num_words, tokenizerFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e99615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWord2Vec(tokenized_list, tokenizer_path, max_length):\n",
    "    \n",
    "    with open(tokenizer_path) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "    \n",
    "    return lines_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "275db8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDLCrossValW2v_multi(X, y, max_len, dim, seed):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train, embedding_matrix, num_words, tokenizerFile = trainWord2Vec(X_train, max_len, dim)\n",
    "        \n",
    "        X_test = testWord2Vec(X_test, tokenizerFile, max_len)\n",
    "        \n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, n_categories)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        \n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = np.argmax(predScores, axis=1)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='macro')\n",
    "        recall=recall_score(Y_test, predictions, average='macro')\n",
    "        f1=f1_score(Y_test, predictions, average='macro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de802e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_w2v == True and multi == True and representation_form == \"sequences\":\n",
    "    dim = 300\n",
    "    runDLCrossValW2v_multi(np.array(tokenized_list), data[\"Category_Index\"].values, 512, dim, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4504cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
