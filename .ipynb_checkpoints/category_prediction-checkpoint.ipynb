{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e24c8",
   "metadata": {},
   "source": [
    "Set the seeder to have as stable random operations as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2010727",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_data = pd.read_csv('bow_data.csv') # bow\n",
    "sequences_data = pd.read_csv('sequences_data.csv') # sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a021b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Vulnerability       Category  Length\n",
      "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9\n",
      "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8\n",
      "2  from django.http import httpresponse, httpresp...  open_redirect       9\n",
      "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175\n",
      "4                          update_query = self.up...  sql_injection      14\n"
     ]
    }
   ],
   "source": [
    "print(sequences_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30bbacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Frequencies:\n",
      " sql_injection            1424\n",
      "xsrf                      976\n",
      "command_injection         721\n",
      "path_disclosure           481\n",
      "open_redirect             442\n",
      "remote_code_execution     334\n",
      "xss                       145\n",
      "Name: Category, dtype: int64\n",
      "Total samples  4523\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = sequences_data['Category'].value_counts()\n",
    "print(\"Label Frequencies:\\n\", label_frequencies)\n",
    "print(\"Total samples \", len(sequences_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d18f",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acbd1",
   "metadata": {},
   "source": [
    "Word2Vec - load pre-trained word2vec embeddings - NL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6946ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "#w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f0d6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ilias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the Punkt tokenizer models if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_list = [word_tokenize(sentence) for sentence in sequences_data[\"Vulnerability\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98aa22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each token using Word2Vec embeddings\n",
    "def w2vEncoding(model, tokenized_list):\n",
    "    encoded_list = []\n",
    "    for sentence_tokens in tokenized_list:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in model:\n",
    "                encoded_token = model[token]\n",
    "                encoded_sentence.append(encoded_token)\n",
    "        encoded_list.append(encoded_sentence)\n",
    "    \n",
    "    return encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae0f0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data[\"w2v\"] = w2vEncoding(w2v_vectors, tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1da388",
   "metadata": {},
   "source": [
    "Use corpus to train word2vec vectors on python source code - PL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f22d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this should be executed only in the training set during cross-validation\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# w2v_model = Word2Vec(sentences=sequences_data[\"Vulnerability\"], vector_size=300, window=5, min_count=1, workers=4)\n",
    "# w2v_model.save(\"python_word2vec.model\")\n",
    "# #w2v_model = Word2Vec.load(\"python_word2vec.model\")\n",
    "# sequences_data[\"py_w2v\"] = w2vEncoding(w2v_model.wv, tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9edbb",
   "metadata": {},
   "source": [
    "BERT - load pre-trained bert embeddings - NL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b99ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e409c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = bert.get_input_embeddings()\n",
    "embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "num_words = len(embedding_matrix)\n",
    "print(num_words)\n",
    "dim = len(embedding_matrix[0])\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c734b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [tokenizer.encode(sente, padding=True, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sequences_data[\"Vulnerability\"]] # Tokenize the complete sentences\n",
    "\n",
    "lines_pad = []\n",
    "for seq in sequences:\n",
    "    lines_pad.append(seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e1621e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data[\"bert\"] = lines_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41a30e",
   "metadata": {},
   "source": [
    "CodeBERT - load pre-trained codebert embeddings - PL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be0cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "codebert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d301684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "codebert_embeddings = codebert.get_input_embeddings()\n",
    "embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "num_words = len(embedding_matrix)\n",
    "print(num_words)\n",
    "dim = len(embedding_matrix[0])\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02135a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sequences_data[\"Vulnerability\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEmbList(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        len_seq = len(seq)\n",
    "        if len_seq < max_len:\n",
    "            for i in range(len_seq, max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    \n",
    "    lines_pad = [arr.tolist() for arr in lines_pad]\n",
    "    \n",
    "    return lines_pad\n",
    "\n",
    "lines_pad = np.array(extractEmbList(sequences, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "747696b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data[\"codebert\"] = lines_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a8fa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vulnerability                f\"str$id\"\"str$id\"\"str$id\"         ...\n",
       "Category                                             sql_injection\n",
       "Length                                                           9\n",
       "w2v              [[-0.21679688, 0.13574219, 0.18652344, 0.11376...\n",
       "bert             [1042, 1000, 2358, 2099, 1002, 8909, 1000, 100...\n",
       "codebert         [1437, 1437, 1437, 1437, 1437, 1437, 1437, 143...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_data.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ce97",
   "metadata": {},
   "source": [
    "RNN model, LSTM specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, multi):\n",
    "    model=Sequential()\n",
    "    #model.add(Embedding(input_dim=top_words+1, output_dim=dim, input_length=None, mask_zero=True))\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(50, dropout=0.1, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    model.add(BatchNormalization(momentum=0.0))\n",
    "    if multi == False:\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')  \n",
    "    else: \n",
    "        pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f9769",
   "metadata": {},
   "source": [
    "CNN model 1-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    if multi == False:\n",
    "        cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "        cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [recall_metric])\n",
    "    else:\n",
    "        pass\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab4fc2",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd95cd0",
   "metadata": {},
   "source": [
    "Binary Classification: Recognition of Injection Vulnerabilities (command_injection and sql_injection merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a654279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for binary evaluation\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9f8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if the category is an injection or not\n",
    "def is_injection(category):\n",
    "    if category in ['sql_injection', 'command_injection']:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "sequences_data['Injection'] = sequences_data['Category'].apply(is_injection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246db4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCrossVal(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'f2']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        Y_train = np.array(Y_train)\n",
    "        Y_train = Y_train.ravel()\n",
    "        Y_test = np.array(Y_test)\n",
    "        Y_test = Y_test.ravel()\n",
    "        \n",
    "        '''#sampling\n",
    "        X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "        #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "        #shuffle dataset\n",
    "        X_resampled=pd.DataFrame(X_res)\n",
    "        Y_resampled=pd.DataFrame(Y_res)\n",
    "        newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "        newTrain = shuffle(newTrain,random_state=seed)\n",
    "        X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "        X_train=pd.DataFrame(X_train)\n",
    "        Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "        Y_train=pd.DataFrame(Y_train)'''\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predictions = (myModel.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        predScores = myModel.predict(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions)\n",
    "        recall=recall_score(Y_test, predictions)\n",
    "        f1=f1_score(Y_test, predictions)\n",
    "        roc_auc=roc_auc_score(Y_test, predictions)\n",
    "        f2=5*precision*recall / (4*precision+recall)\n",
    "        print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "        print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "        score_dict['f2'] = np.append(score_dict['f2'], f2)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "    print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))\n",
    "    print(\"f2: %.2f%% (%.2f%%)\" % (score_dict['f2'].mean()*100, score_dict['f2'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "runCrossVal(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3dea",
   "metadata": {},
   "source": [
    "Multi-class Classification: Categorization of all detected vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f4d62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for multiclass evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e8cf6",
   "metadata": {},
   "source": [
    "Use ML models and BoW code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fd054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d28251b1",
   "metadata": {},
   "source": [
    "Use DL modles and sequences of tokens code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e21ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
