{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e24c8",
   "metadata": {},
   "source": [
    "Set the seeder to have as stable random operations as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2010727",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_data = pd.read_csv('bow_data.csv') # bow\n",
    "sequences_data = pd.read_csv('sequences_data.csv') # sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a021b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Vulnerability       Category  Length\n",
      "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9\n",
      "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8\n",
      "2  from django.http import httpresponse, httpresp...  open_redirect       9\n",
      "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175\n",
      "4                          update_query = self.up...  sql_injection      14\n"
     ]
    }
   ],
   "source": [
    "print(sequences_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "236e84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Frequencies:\n",
      " sql_injection            1424\n",
      "xsrf                      976\n",
      "command_injection         721\n",
      "path_disclosure           481\n",
      "open_redirect             442\n",
      "remote_code_execution     334\n",
      "xss                       145\n",
      "Name: Category, dtype: int64\n",
      "Total samples  4523\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = sequences_data['Category'].value_counts()\n",
    "print(\"Label Frequencies:\\n\", label_frequencies)\n",
    "print(\"Total samples \", len(sequences_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d18f",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acbd1",
   "metadata": {},
   "source": [
    "Word2Vec - load pre-trained word2vec embeddings - NL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6946ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "#w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3202c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ilias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the Punkt tokenizer models if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_list = [word_tokenize(sentence) for sentence in sequences_data[\"Vulnerability\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "819ac4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each token using Word2Vec embeddings\n",
    "def w2vEncoding(model, tokenized_list):\n",
    "    encoded_list = []\n",
    "    for sentence_tokens in tokenized_list:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in model:\n",
    "                encoded_token = model[token]\n",
    "                encoded_sentence.append(encoded_token)\n",
    "        encoded_list.append(encoded_sentence)\n",
    "    \n",
    "    return encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47f831e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data[\"w2v\"] = w2vEncoding(w2v_vectors, tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1da388",
   "metadata": {},
   "source": [
    "Use corpus to train word2vec vectors on python source code - PL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f22d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this should be executed only in the training set during cross-validation\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# w2v_model = Word2Vec(sentences=sequences_data[\"Vulnerability\"], vector_size=300, window=5, min_count=1, workers=4)\n",
    "# w2v_model.save(\"python_word2vec.model\")\n",
    "# #w2v_model = Word2Vec.load(\"python_word2vec.model\")\n",
    "# sequences_data[\"py_w2v\"] = w2vEncoding(w2v_model.wv, tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31e8ce",
   "metadata": {},
   "source": [
    "BERT - load pre-trained bert embeddings - NL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4c418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12266dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = bert.get_input_embeddings()\n",
    "embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "num_words = len(embedding_matrix)\n",
    "print(num_words)\n",
    "dim = len(embedding_matrix[0])\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23094722",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [tokenizer.encode(sente, padding=True, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sequences_data[\"Vulnerability\"]] # Tokenize the complete sentences\n",
    "\n",
    "lines_pad = []\n",
    "for seq in sequences:\n",
    "    lines_pad.append(seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa48c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data[\"bert\"] = lines_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41a30e",
   "metadata": {},
   "source": [
    "CodeBERT - load pre-trained codebert embeddings - PL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be0cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "codebert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db02fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "codebert_embeddings = codebert.get_input_embeddings()\n",
    "embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "num_words = len(embedding_matrix)\n",
    "print(num_words)\n",
    "dim = len(embedding_matrix[0])\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f89045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [tokenizer(sente, return_tensors=\"tf\", padding=True, truncation=True, add_special_tokens=False) for sente in sequences_data[\"Vulnerability\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6453c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilias\\AppData\\Local\\Temp\\ipykernel_9464\\1520574951.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  lines_pad = np.array(extractEmbList(sequences))\n"
     ]
    }
   ],
   "source": [
    "def extractEmbList(sequences):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "#         if len(seq) < max_len:\n",
    "#             for i in range(len(seq), max_len):\n",
    "#                 seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    \n",
    "    [arr.tolist() for arr in lines_pad]\n",
    "    \n",
    "    return lines_pad\n",
    "\n",
    "lines_pad = np.array(extractEmbList(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd6539a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_data[\"codebert\"] = lines_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdecac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vulnerability                f\"str$id\"\"str$id\"\"str$id\"         ...\n",
       "Category                                             sql_injection\n",
       "Length                                                           9\n",
       "w2v              [[-0.21679688, 0.13574219, 0.18652344, 0.11376...\n",
       "bert             [1042, 1000, 2358, 2099, 1002, 8909, 1000, 100...\n",
       "codebert         [1437, 1437, 1437, 1437, 1437, 1437, 1437, 143...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_data.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ce97",
   "metadata": {},
   "source": [
    "RNN model, LSTM specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, multi):\n",
    "    model=Sequential()\n",
    "    #model.add(Embedding(input_dim=top_words+1, output_dim=dim, input_length=None, mask_zero=True))\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(50, dropout=0.1, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    model.add(BatchNormalization(momentum=0.0))\n",
    "    if multi == False:\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')  \n",
    "    else: \n",
    "        pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f9769",
   "metadata": {},
   "source": [
    "CNN model 1-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    if multi == False:\n",
    "        cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "        cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [recall_metric])\n",
    "    else:\n",
    "        pass\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab4fc2",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8207c8f",
   "metadata": {},
   "source": [
    "Binary Classification: Recognition of Injection Vulnerabilities (command_injection and sql_injection merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b846cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if the category is an injection or not\n",
    "def is_injection(category):\n",
    "    if category in ['sql_injection', 'command_injection']:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "sequences_data['Injection'] = sequences_data['Category'].apply(is_injection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c515c9c",
   "metadata": {},
   "source": [
    "Multi-class Classification: Categorization of all detected vulnerabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e48a00",
   "metadata": {},
   "source": [
    "Use ML models and BoW code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab476c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0cf44ba",
   "metadata": {},
   "source": [
    "Use DL modles and sequences of tokens code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bface",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
