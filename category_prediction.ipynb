{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader\n",
    "import io\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e24c8",
   "metadata": {},
   "source": [
    "Set the seeder to have as stable random operations as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a6448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_algorithms = [\"w2v\", \"ft\", \"ft_code\", \"bert\", \"codebert\"]\n",
    "embedding_algorithm = embedding_algorithms[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f9b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"bow\", \"sequences\"]\n",
    "representation_form = representations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3acc7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary or multi-class\n",
    "multi = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a56a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2010727",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if representation_form == \"bow\":\n",
    "    data = pd.read_csv('bow_data.csv') # bow\n",
    "else:\n",
    "    data = pd.read_csv('sequences_data.csv') # sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abc9724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vulnerability</th>\n",
       "      <th>Category</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f\"str$id\"\"str$id\"\"str$id\"         ...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client.listentcp()    proxy = proxy(proxy_...</td>\n",
       "      <td>xsrf</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from django.http import httpresponse, httpresp...</td>\n",
       "      <td>open_redirect</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def write_preset(conn, queryin, descriptin):\\t...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>update_query = self.up...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Vulnerability       Category  Length\n",
       "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9\n",
       "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8\n",
       "2  from django.http import httpresponse, httpresp...  open_redirect       9\n",
       "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175\n",
       "4                          update_query = self.up...  sql_injection      14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5d5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if representation_form == \"bow\":\n",
    "    bow_size = 237 # number of columns that stand as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e46cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48846336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 392\n"
     ]
    }
   ],
   "source": [
    "if representation_form == \"sequences\":\n",
    "    word_counts = data[\"Vulnerability\"].apply(lambda x: len(x.split()))\n",
    "    max_length = word_counts.max()\n",
    "    print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bbacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Frequencies:\n",
      " sql_injection            1424\n",
      "xsrf                      976\n",
      "command_injection         721\n",
      "path_disclosure           481\n",
      "open_redirect             442\n",
      "remote_code_execution     334\n",
      "xss                       145\n",
      "Name: Category, dtype: int64\n",
      "Total samples  4523\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = data['Category'].value_counts()\n",
    "print(\"Label Frequencies:\\n\", label_frequencies)\n",
    "print(\"Total samples \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b777af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True:\n",
    "    n_categories = len(label_frequencies) # 7\n",
    "    n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d18f",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acbd1",
   "metadata": {},
   "source": [
    "Word2Vec - load pre-trained word2vec embeddings - NL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0d6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaskaloup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if representation_form == \"sequences\":\n",
    "\n",
    "    # Download the Punkt tokenizer models if not already downloaded\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    tokenized_list = [word_tokenize(sentence) for sentence in data[\"Vulnerability\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0f0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"w2v\" and representation_form == \"sequences\" and train_w2v == False:\n",
    "\n",
    "    w2v_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "    dim = w2v_vectors.vector_size\n",
    "\n",
    "#     fileEmb = 'w2v_embeddingsIters.txt'\n",
    "#     w2v_vectors.save_word2vec_format(fileEmb, binary=False)\n",
    "    \n",
    "#     embeddings_index = {}\n",
    "#     f = open(os.path.join('', fileEmb), encoding=\"utf-8\")\n",
    "#     for line in f:    \n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:])\n",
    "#         embeddings_index[word] = coefs   \n",
    "#     f.close()\n",
    "    \n",
    "    \n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "    \n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'w2v_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "    \n",
    "    \n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = w2v_vectors[word] if word in w2v_vectors else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c4e52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"ft\" and representation_form == \"sequences\":\n",
    "\n",
    "    fastText_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "    dim = fastText_vectors.vector_size   \n",
    "    \n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "    \n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'fast_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)    \n",
    "    \n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = fastText_vectors[word] if word in fastText_vectors else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8f31f",
   "metadata": {},
   "source": [
    "Pre-trained FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9794262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"ft_code\" and representation_form == \"sequences\":\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('', 'fast-text_embeddings.txt'), encoding=\"utf-8\")\n",
    "    for line in f:    \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:])\n",
    "        embeddings_index[word] = coefs   \n",
    "    f.close() \n",
    "    \n",
    "    dim = 100\n",
    "    \n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "    \n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'fastCode_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)    \n",
    "    \n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9edbb",
   "metadata": {},
   "source": [
    "BERT - load pre-trained bert embeddings - NL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6b99ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"bert\" and representation_form == \"sequences\": \n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)\n",
    "    \n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c734b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"bert\" and representation_form == \"sequences\":\n",
    "    sentences = data[\"Vulnerability\"].tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "    \n",
    "    lines_pad = []\n",
    "    for seq in sequences:\n",
    "        lines_pad.append(seq[0])\n",
    "    \n",
    "    lines_pad = pad_sequences(lines_pad, padding = 'post', maxlen = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41a30e",
   "metadata": {},
   "source": [
    "CodeBERT - load pre-trained codebert embeddings - PL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be0cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"codebert\" and representation_form == \"sequences\": \n",
    "    model_variation = \"microsoft/codebert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation)\n",
    "    \n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f596d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02135a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"codebert\" and representation_form == \"sequences\": \n",
    "\n",
    "    sentences = data[\"Vulnerability\"].tolist()\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]\n",
    "    \n",
    "    lines_pad = padSequences(sequences, 512)\n",
    "    lines_pad = [arr.tolist() for arr in lines_pad]\n",
    "    lines_pad = np.array(lines_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ce97",
   "metadata": {},
   "source": [
    "RNN model, LSTM specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7222be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, multi, n_outputs):\n",
    "    model=Sequential()\n",
    "    #model.add(Embedding(input_dim=top_words+1, output_dim=dim, input_length=None, mask_zero=True))\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(50, dropout=0.1, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    model.add(BatchNormalization(momentum=0.0))\n",
    "    if multi == False:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')  \n",
    "    else: \n",
    "        model.add(Dense(n_outputs, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f9769",
   "metadata": {},
   "source": [
    "CNN model 1-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "688dbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix, multi, n_outputs):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 1, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    if multi == False:\n",
    "        cnn_model.add(Dense(n_outputs, activation = 'sigmoid'))\n",
    "        cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\")\n",
    "    else: \n",
    "        model.add(Dense(n_outputs, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab4fc2",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd95cd0",
   "metadata": {},
   "source": [
    "Binary Classification: Recognition of Injection Vulnerabilities (command_injection and sql_injection merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab9f8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if the category is an injection or not\n",
    "def is_injection(category):\n",
    "    if category in ['sql_injection', 'command_injection']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "if multi == False:\n",
    "    data['Injection'] = data['Category'].apply(is_injection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcdff955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW and ML\n",
    "\n",
    "def runMLCrossVal_binary(X, y, seed, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"svm\":\n",
    "            myModel = SVC(kernel='rbf', gamma=100)\n",
    "        elif userModel == \"RF\":\n",
    "            myModel = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt')\n",
    "        elif userModel == \"DT\":\n",
    "            myModel = tree.DecisionTreeClassifier(max_depth=120)\n",
    "        elif userModel == \"NB\":\n",
    "            myModel = GaussianNB()\n",
    "\n",
    "        myModel.fit(X_train, Y_train.ravel())\n",
    "        \n",
    "        predictions = myModel.predict(X_test)\n",
    "        #predScores = myModel.predict_proba(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions)\n",
    "        recall=recall_score(Y_test, predictions)\n",
    "        f1=f1_score(Y_test, predictions)\n",
    "        roc_auc=roc_auc_score(Y_test, predictions)\n",
    "        print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "    print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4da8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == False and representation_form == \"bow\":\n",
    "    runMLCrossVal_binary(data.iloc[:, 0:bow_size], data[\"Injection\"], seed, \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "246db4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences of tokens and DL\n",
    "\n",
    "def runDLCrossVal_binary(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel, multi):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, multi, 1) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, multi, 1)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predictions = (myModel.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        predScores = myModel.predict(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions)\n",
    "        recall=recall_score(Y_test, predictions)\n",
    "        f1=f1_score(Y_test, predictions)\n",
    "        roc_auc=roc_auc_score(Y_test, predictions)\n",
    "        print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "    print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced8e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == False and representation_form == \"sequences\":\n",
    "    runDLCrossVal_binary(lines_pad, data[\"Injection\"].values, 512, num_words, dim, seed, embedding_matrix, \"lstm\", multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3dea",
   "metadata": {},
   "source": [
    "Multi-class Classification: Categorization of all detected vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab8dd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True:\n",
    "    \n",
    "    # Convert categories to numerical indexes\n",
    "    category_numerical_indexes, unique_categories = data[\"Category\"].factorize()\n",
    "\n",
    "    # Create a dictionary mapping each category to its numerical index\n",
    "    category_to_index = {category: index for index, category in enumerate(unique_categories)}\n",
    "\n",
    "    # Update the categories in the DataFrame with their numerical indexes\n",
    "    data[\"Category_Index\"] = data[\"Category\"].map(category_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e8cf6",
   "metadata": {},
   "source": [
    "Use ML models and BoW code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "613fd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMLCrossVal_multi(X, y, seed, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"svm\":\n",
    "            myModel = SVC(kernel='rbf', gamma=100)\n",
    "        elif userModel == \"RF\":\n",
    "            myModel = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt')\n",
    "        elif userModel == \"DT\":\n",
    "            myModel = tree.DecisionTreeClassifier(max_depth=120)\n",
    "        elif userModel == \"NB\":\n",
    "            myModel = GaussianNB()\n",
    "            \n",
    "        myModel.fit(X_train, Y_train.ravel())\n",
    "        \n",
    "        predictions = myModel.predict(X_test)\n",
    "        #predScores = myModel.predict_proba(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='macro')\n",
    "        recall=recall_score(Y_test, predictions, average='macro')\n",
    "        f1=f1_score(Y_test, predictions, average='macro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        #tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        #acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8488cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True and representation_form == \"bow\":\n",
    "    runMLCrossVal_multi(data.iloc[:, 0:bow_size], data[\"Category_Index\"], seed, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28251b1",
   "metadata": {},
   "source": [
    "Use DL models and sequences of tokens code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "881e21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDLCrossVal_multi(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel, multi):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = np.argmax(predScores, axis=1)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='macro')\n",
    "        recall=recall_score(Y_test, predictions, average='macro')\n",
    "        f1=f1_score(Y_test, predictions, average='macro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd4ae015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "fold number=  1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 12s 58ms/step - loss: 1.7058 - val_loss: 1.4172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41716, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 29ms/step - loss: 1.3734 - val_loss: 1.3146\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.41716 to 1.31462, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.2504 - val_loss: 1.2341\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.31462 to 1.23412, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.1645 - val_loss: 1.2303\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.23412 to 1.23025, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0762 - val_loss: 1.1769\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.23025 to 1.17686, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0211 - val_loss: 1.1608\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.17686 to 1.16083, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9712 - val_loss: 1.1102\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.16083 to 1.11016, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9087 - val_loss: 1.0951\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.11016 to 1.09514, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8444 - val_loss: 1.0599\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.09514 to 1.05994, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8052 - val_loss: 1.0830\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.05994\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7506 - val_loss: 1.0970\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.05994\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7223 - val_loss: 1.1256\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.05994\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6824 - val_loss: 1.0769\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.05994\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6503 - val_loss: 1.1197\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.05994\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6088 - val_loss: 1.1143\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.05994\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5862 - val_loss: 1.1295\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05994\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5655 - val_loss: 1.1150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05994\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5474 - val_loss: 1.1691\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05994\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5099 - val_loss: 1.1175\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05994\n",
      "Epoch 00019: early stopping\n",
      "[[115   5   4   0   6   5   8]\n",
      " [ 19  64   5   0   1   6   2]\n",
      " [  6  10  20   0   3   3   3]\n",
      " [  5   2   1   2   2   0   2]\n",
      " [  6   4   1   0  20   0   3]\n",
      " [ 10   4   5   0   1  50   2]\n",
      " [  5   4   2   0   3   3  31]]\n",
      "Accuracy:66.67%\n",
      "Precision:66.67%\n",
      "Recall:66.67%\n",
      "F1 score:66.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74       143\n",
      "           1       0.69      0.66      0.67        97\n",
      "           2       0.53      0.44      0.48        45\n",
      "           3       1.00      0.14      0.25        14\n",
      "           4       0.56      0.59      0.57        34\n",
      "           5       0.75      0.69      0.72        72\n",
      "           6       0.61      0.65      0.63        48\n",
      "\n",
      "    accuracy                           0.67       453\n",
      "   macro avg       0.69      0.57      0.58       453\n",
      "weighted avg       0.67      0.67      0.66       453\n",
      "\n",
      "fold number=  2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 51ms/step - loss: 1.7144 - val_loss: 1.3948\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.39479, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.4110 - val_loss: 1.2453\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.39479 to 1.24531, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.2691 - val_loss: 1.1961\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.24531 to 1.19610, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.1793 - val_loss: 1.1117\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.19610 to 1.11168, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0985 - val_loss: 1.1087\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.11168 to 1.10871, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0468 - val_loss: 1.0661T\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.10871 to 1.06605, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9694 - val_loss: 1.0229\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.06605 to 1.02292, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9177 - val_loss: 0.9939\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.02292 to 0.99393, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8449 - val_loss: 0.9775\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99393 to 0.97745, saving model to best_model.h5\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8163 - val_loss: 0.9518\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.97745 to 0.95184, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7754 - val_loss: 0.9454\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.95184 to 0.94542, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7264 - val_loss: 0.9800 0s - loss: 0 - ETA: 0s - loss: 0\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.94542\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6996 - val_loss: 0.9860\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.94542\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6701 - val_loss: 0.9033\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.94542 to 0.90333, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6398 - val_loss: 0.9176\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.90333\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6033 - val_loss: 0.8719\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.90333 to 0.87189, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5834 - val_loss: 0.9070\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.87189\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5608 - val_loss: 0.9463\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.87189\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5378 - val_loss: 0.8850\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.87189\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5209 - val_loss: 0.8941\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.87189\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4984 - val_loss: 0.8674\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.87189 to 0.86736, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4825 - val_loss: 0.8926\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.86736\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4780 - val_loss: 0.8699\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.86736\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4606 - val_loss: 0.8750\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.86736\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4307 - val_loss: 0.8519\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.86736 to 0.85190, saving model to best_model.h5\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4160 - val_loss: 0.9089\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.85190\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4079 - val_loss: 0.9039\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.85190\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3939 - val_loss: 0.8908\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.85190\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3852 - val_loss: 0.9018\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.85190\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3646 - val_loss: 0.9124\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.85190\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3608 - val_loss: 0.9137\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.85190\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3632 - val_loss: 0.9499\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.85190\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3481 - val_loss: 0.9480\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.85190\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3446 - val_loss: 0.8719\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.85190\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3365 - val_loss: 0.8740\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.85190\n",
      "Epoch 00035: early stopping\n",
      "[[122   1   6   0   2   5   7]\n",
      " [ 11  70   2   0   1   6   7]\n",
      " [  9   5  29   0   1   0   1]\n",
      " [  4   0   0   9   0   1   0]\n",
      " [  5   2   2   0  24   0   0]\n",
      " [  5   3   2   1   4  57   1]\n",
      " [  6   5   2   1   2   4  28]]\n",
      "Accuracy:74.83%\n",
      "Precision:74.83%\n",
      "Recall:74.83%\n",
      "F1 score:74.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80       143\n",
      "           1       0.81      0.72      0.77        97\n",
      "           2       0.67      0.64      0.66        45\n",
      "           3       0.82      0.64      0.72        14\n",
      "           4       0.71      0.73      0.72        33\n",
      "           5       0.78      0.78      0.78        73\n",
      "           6       0.64      0.58      0.61        48\n",
      "\n",
      "    accuracy                           0.75       453\n",
      "   macro avg       0.74      0.71      0.72       453\n",
      "weighted avg       0.75      0.75      0.75       453\n",
      "\n",
      "fold number=  3\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 53ms/step - loss: 1.7438 - val_loss: 1.5144\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.51436, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.3871 - val_loss: 1.4283\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.51436 to 1.42833, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.2553 - val_loss: 1.3478A: 0s - loss: 1\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42833 to 1.34785, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.1722 - val_loss: 1.3331\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.34785 to 1.33313, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0809 - val_loss: 1.2894\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33313 to 1.28937, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0256 - val_loss: 1.2816\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28937 to 1.28156, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9671 - val_loss: 1.2078s - loss: 0.96\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.28156 to 1.20781, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9197 - val_loss: 1.1613\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.20781 to 1.16132, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8788 - val_loss: 1.1204\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.16132 to 1.12043, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8345 - val_loss: 1.1106\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.12043 to 1.11055, saving model to best_model.h5\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7982 - val_loss: 1.1093\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.11055 to 1.10933, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7532 - val_loss: 1.1240\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.10933\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7120 - val_loss: 1.0590\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.10933 to 1.05895, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6864 - val_loss: 1.0722- ETA: 0s - loss\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.05895\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6608 - val_loss: 1.0890\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.05895\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6302 - val_loss: 1.1098\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05895\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6038 - val_loss: 1.0840\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05895\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5678 - val_loss: 1.1296\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05895\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5552 - val_loss: 1.0916\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05895\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5306 - val_loss: 1.1246oss: \n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.05895\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5174 - val_loss: 1.1300\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.05895\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4982 - val_loss: 1.1164\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.05895\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4796 - val_loss: 1.1465\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.05895\n",
      "Epoch 00023: early stopping\n",
      "[[113   3   9   1   3   7   7]\n",
      " [ 14  73   3   0   3   2   2]\n",
      " [ 14   6  19   0   1   3   1]\n",
      " [  2   3   1   4   2   2   1]\n",
      " [  2   1   8   0  16   3   3]\n",
      " [ 17   2   5   0   2  44   2]\n",
      " [  9   7   1   1   2   1  28]]\n",
      "Accuracy:65.56%\n",
      "Precision:65.56%\n",
      "Recall:65.56%\n",
      "F1 score:65.56%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72       143\n",
      "           1       0.77      0.75      0.76        97\n",
      "           2       0.41      0.43      0.42        44\n",
      "           3       0.67      0.27      0.38        15\n",
      "           4       0.55      0.48      0.52        33\n",
      "           5       0.71      0.61      0.66        72\n",
      "           6       0.64      0.57      0.60        49\n",
      "\n",
      "    accuracy                           0.66       453\n",
      "   macro avg       0.63      0.56      0.58       453\n",
      "weighted avg       0.66      0.66      0.65       453\n",
      "\n",
      "fold number=  4\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 55ms/step - loss: 1.7241 - val_loss: 1.3748\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.37479, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.3806 - val_loss: 1.2314\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.37479 to 1.23135, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.2425 - val_loss: 1.1570\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.23135 to 1.15702, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.1359 - val_loss: 1.1074\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.15702 to 1.10744, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0682 - val_loss: 1.0670\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.10744 to 1.06704, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9998 - val_loss: 1.0505\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.06704 to 1.05053, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.9371 - val_loss: 1.0265\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.05053 to 1.02654, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8811 - val_loss: 1.0240\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.02654 to 1.02401, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8316 - val_loss: 1.0179\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.02401 to 1.01793, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7867 - val_loss: 0.9902\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.01793 to 0.99019, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7571 - val_loss: 1.0043\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99019\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7172 - val_loss: 1.0016 0s - loss: \n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99019\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6660 - val_loss: 1.0087\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99019\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6521 - val_loss: 0.9787\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99019 to 0.97866, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6119 - val_loss: 0.9808\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.97866\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5744 - val_loss: 0.9960s - loss: 0.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.97866\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5710 - val_loss: 1.0122A: 0s\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.97866\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5472 - val_loss: 0.9800\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.97866\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5131 - val_loss: 0.9954\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.97866\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4951 - val_loss: 0.9747- ETA: 0s - loss: \n",
      "\n",
      "Epoch 00020: val_loss improved from 0.97866 to 0.97474, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4758 - val_loss: 1.0007\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.97474\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4638 - val_loss: 1.0392ET\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.97474\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4376 - val_loss: 1.0633\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.97474\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4356 - val_loss: 1.0254 0\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.97474\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4218 - val_loss: 1.0285\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.97474\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4132 - val_loss: 1.0360\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.97474\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4033 - val_loss: 1.0554: 0s - loss: 0 - ETA: 0s - loss: \n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.97474\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.3891 - val_loss: 1.0243\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.97474\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.3774 - val_loss: 1.0389\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.97474\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.3693 - val_loss: 1.0246\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.97474\n",
      "Epoch 00030: early stopping\n",
      "[[119   5   6   1   2   4   6]\n",
      " [  5  79   7   2   0   1   3]\n",
      " [  5   6  26   0   0   1   6]\n",
      " [  5   2   4   2   0   0   2]\n",
      " [  3   3   0   2  22   2   1]\n",
      " [  8   6   2   1   1  52   2]\n",
      " [  5   2   2   2   2   4  31]]\n",
      "Accuracy:73.23%\n",
      "Precision:73.23%\n",
      "Recall:73.23%\n",
      "F1 score:73.23%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       143\n",
      "           1       0.77      0.81      0.79        97\n",
      "           2       0.55      0.59      0.57        44\n",
      "           3       0.20      0.13      0.16        15\n",
      "           4       0.81      0.67      0.73        33\n",
      "           5       0.81      0.72      0.76        72\n",
      "           6       0.61      0.65      0.63        48\n",
      "\n",
      "    accuracy                           0.73       452\n",
      "   macro avg       0.65      0.63      0.64       452\n",
      "weighted avg       0.73      0.73      0.73       452\n",
      "\n",
      "fold number=  5\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 52ms/step - loss: 1.7520 - val_loss: 1.3961\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.39613, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.3869 - val_loss: 1.2435\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.39613 to 1.24346, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.2592 - val_loss: 1.17511.2\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.24346 to 1.17513, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.1745 - val_loss: 1.1137\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.17513 to 1.11367, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0799 - val_loss: 1.0782\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.11367 to 1.07819, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0190 - val_loss: 1.0533ETA: 0s - loss: 1.0\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.07819 to 1.05331, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.9556 - val_loss: 1.0231\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.05331 to 1.02315, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9151 - val_loss: 1.0047\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.02315 to 1.00469, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8663 - val_loss: 0.9724\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00469 to 0.97242, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8062 - val_loss: 1.0143TA\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.97242\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7528 - val_loss: 1.0231\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.97242\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7355 - val_loss: 1.0250\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.97242\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7063 - val_loss: 0.9818s - loss: \n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.97242\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6764 - val_loss: 0.9919\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.97242\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6466 - val_loss: 0.9758\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.97242\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6044 - val_loss: 1.0309\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.97242\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5812 - val_loss: 0.9709A: 0\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.97242 to 0.97085, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5567 - val_loss: 0.9303\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.97085 to 0.93032, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5318 - val_loss: 0.9900A: 0s\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.93032\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5227 - val_loss: 0.9963\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.93032\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5109 - val_loss: 0.9761\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.93032\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4904 - val_loss: 0.9989 0\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.93032\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4774 - val_loss: 1.0475\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.93032\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4594 - val_loss: 1.0312\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.93032\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4689 - val_loss: 0.9972\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.93032\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4407 - val_loss: 0.9899\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.93032\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4233 - val_loss: 1.0156\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.93032\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4081 - val_loss: 0.9967\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.93032\n",
      "Epoch 00028: early stopping\n",
      "[[116   8   3   2   4   6   3]\n",
      " [  5  74   5   0   1   4   9]\n",
      " [ 10   6  24   0   2   1   1]\n",
      " [  3   2   0   6   0   1   3]\n",
      " [  4   0   3   0  21   4   1]\n",
      " [  4   4   1   1   3  55   4]\n",
      " [  5   6   3   1   2   3  28]]\n",
      "Accuracy:71.68%\n",
      "Precision:71.68%\n",
      "Recall:71.68%\n",
      "F1 score:71.68%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       142\n",
      "           1       0.74      0.76      0.75        98\n",
      "           2       0.62      0.55      0.58        44\n",
      "           3       0.60      0.40      0.48        15\n",
      "           4       0.64      0.64      0.64        33\n",
      "           5       0.74      0.76      0.75        72\n",
      "           6       0.57      0.58      0.58        48\n",
      "\n",
      "    accuracy                           0.72       452\n",
      "   macro avg       0.67      0.64      0.65       452\n",
      "weighted avg       0.71      0.72      0.71       452\n",
      "\n",
      "fold number=  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 51ms/step - loss: 1.7572 - val_loss: 1.4309ETA: 1s - l - ETA: 0s - loss\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43086, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.3911 - val_loss: 1.3224- loss:\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43086 to 1.32244, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.2507 - val_loss: 1.2442\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32244 to 1.24421, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.1581 - val_loss: 1.1849- ETA: 1s - loss: 1.1 - ETA\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24421 to 1.18489, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0868 - val_loss: 1.1053\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.18489 to 1.10525, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0162 - val_loss: 1.1023\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.10525 to 1.10226, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.9445 - val_loss: 1.0640\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.10226 to 1.06403, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8944 - val_loss: 1.0687\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.06403\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8461 - val_loss: 1.0338\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.06403 to 1.03384, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7903 - val_loss: 1.0223: 0s\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.03384 to 1.02234, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7723 - val_loss: 1.0323\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.02234\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7249 - val_loss: 1.0177\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.02234 to 1.01769, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6934 - val_loss: 0.9984\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.01769 to 0.99838, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6502 - val_loss: 1.0134\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99838\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6264 - val_loss: 1.0362\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.99838\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5938 - val_loss: 1.0472\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.99838\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5510 - val_loss: 1.0517\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.99838\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5548 - val_loss: 1.0407\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.99838\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5269 - val_loss: 1.0423\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.99838\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5112 - val_loss: 1.0654\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.99838\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4960 - val_loss: 1.0771\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.99838\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4795 - val_loss: 1.0737\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.99838\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4588 - val_loss: 1.0868\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.99838\n",
      "Epoch 00023: early stopping\n",
      "[[119   9   4   0   1   3   6]\n",
      " [ 13  71   5   0   3   3   3]\n",
      " [ 10   4  24   1   2   3   0]\n",
      " [  2   2   2   3   1   2   3]\n",
      " [  9   6   0   0  16   0   2]\n",
      " [ 10   5   0   1   2  52   2]\n",
      " [  5   6   1   0   0   4  32]]\n",
      "Accuracy:70.13%\n",
      "Precision:70.13%\n",
      "Recall:70.13%\n",
      "F1 score:70.13%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.84      0.77       142\n",
      "           1       0.69      0.72      0.71        98\n",
      "           2       0.67      0.55      0.60        44\n",
      "           3       0.60      0.20      0.30        15\n",
      "           4       0.64      0.48      0.55        33\n",
      "           5       0.78      0.72      0.75        72\n",
      "           6       0.67      0.67      0.67        48\n",
      "\n",
      "    accuracy                           0.70       452\n",
      "   macro avg       0.68      0.60      0.62       452\n",
      "weighted avg       0.70      0.70      0.69       452\n",
      "\n",
      "fold number=  7\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 55ms/step - loss: 1.7351 - val_loss: 1.4449\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.44490, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.4004 - val_loss: 1.3070\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.44490 to 1.30698, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.2663 - val_loss: 1.2086\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30698 to 1.20864, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.1585 - val_loss: 1.1599\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.20864 to 1.15994, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0749 - val_loss: 1.1067\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.15994 to 1.10670, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0255 - val_loss: 1.0342\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.10670 to 1.03418, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9559 - val_loss: 1.0195\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.03418 to 1.01946, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8941 - val_loss: 1.0015\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.01946 to 1.00150, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8481 - val_loss: 1.0176\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00150\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8090 - val_loss: 1.0042- ETA: 0s - loss:\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00150\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7571 - val_loss: 1.0044\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.00150\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7227 - val_loss: 1.0023\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.00150\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6800 - val_loss: 1.0030\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.00150\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6542 - val_loss: 1.0170A: 1s -  - ETA: 0s - loss: 0.65 - ETA: 0s - loss: 0.6 - ETA: 0s - loss: 0.654\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.00150\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6413 - val_loss: 0.9630\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.00150 to 0.96303, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5956 - val_loss: 0.9707\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.96303\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5800 - val_loss: 0.9588\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.96303 to 0.95882, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5873 - val_loss: 0.9791\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.95882\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5511 - val_loss: 0.9469\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.95882 to 0.94688, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5225 - val_loss: 0.9763\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.94688\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4908 - val_loss: 0.9489oss:\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.94688\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4730 - val_loss: 0.9630\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.94688\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4568 - val_loss: 0.9499\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.94688\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4650 - val_loss: 0.9562\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.94688\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4383 - val_loss: 0.9927\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.94688\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4154 - val_loss: 0.9785\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.94688\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4130 - val_loss: 0.9583\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.94688\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.3957 - val_loss: 1.0074\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.94688\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.3839 - val_loss: 0.9757\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.94688\n",
      "Epoch 00029: early stopping\n",
      "[[117   9   5   1   2   2   6]\n",
      " [  9  73   3   1   4   6   2]\n",
      " [  6   4  28   1   2   2   1]\n",
      " [  1   2   1   6   3   1   1]\n",
      " [  3   2   2   0  17   6   3]\n",
      " [ 10   4   0   1   1  55   1]\n",
      " [  4   7   5   1   1   1  29]]\n",
      "Accuracy:71.90%\n",
      "Precision:71.90%\n",
      "Recall:71.90%\n",
      "F1 score:71.90%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       142\n",
      "           1       0.72      0.74      0.73        98\n",
      "           2       0.64      0.64      0.64        44\n",
      "           3       0.55      0.40      0.46        15\n",
      "           4       0.57      0.52      0.54        33\n",
      "           5       0.75      0.76      0.76        72\n",
      "           6       0.67      0.60      0.64        48\n",
      "\n",
      "    accuracy                           0.72       452\n",
      "   macro avg       0.67      0.64      0.65       452\n",
      "weighted avg       0.71      0.72      0.72       452\n",
      "\n",
      "fold number=  8\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 9s 53ms/step - loss: 1.7415 - val_loss: 1.4895\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48953, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.4040 - val_loss: 1.3257 0s - loss: \n",
      "\n",
      "Epoch 00002: val_loss improved from 1.48953 to 1.32566, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.2625 - val_loss: 1.2425\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32566 to 1.24253, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.1697 - val_loss: 1.2196\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24253 to 1.21956, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0902 - val_loss: 1.1941\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.21956 to 1.19412, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0389 - val_loss: 1.1582\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.19412 to 1.15819, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9846 - val_loss: 1.1095\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.15819 to 1.10953, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.9179 - val_loss: 1.1081\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.10953 to 1.10811, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8677 - val_loss: 1.1071\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.10811 to 1.10714, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8342 - val_loss: 1.0898\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.10714 to 1.08984, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7980 - val_loss: 1.1248\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.08984\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7542 - val_loss: 1.0729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 1.08984 to 1.07287, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7148 - val_loss: 1.1000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.07287\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6826 - val_loss: 1.0806\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.07287\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6605 - val_loss: 1.0534\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.07287 to 1.05337, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6230 - val_loss: 1.0653\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05337\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5938 - val_loss: 1.0632\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05337\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5708 - val_loss: 1.1023\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05337\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5603 - val_loss: 1.1148\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05337\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5299 - val_loss: 1.0631\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.05337\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 29ms/step - loss: 0.5345 - val_loss: 1.1010\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.05337\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4994 - val_loss: 1.1105\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.05337\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4796 - val_loss: 1.1270\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.05337\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4798 - val_loss: 1.1676\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.05337\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4458 - val_loss: 1.1472\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.05337\n",
      "Epoch 00025: early stopping\n",
      "[[111  11   6   0   1   4   9]\n",
      " [  3  75   4   0   0   9   7]\n",
      " [  6   5  24   0   3   3   3]\n",
      " [  4   2   1   2   0   4   1]\n",
      " [  0   4   2   1  21   2   4]\n",
      " [  7   8   2   0   2  48   5]\n",
      " [  5   8   2   0   2   0  31]]\n",
      "Accuracy:69.03%\n",
      "Precision:69.03%\n",
      "Recall:69.03%\n",
      "F1 score:69.03%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       142\n",
      "           1       0.66      0.77      0.71        98\n",
      "           2       0.59      0.55      0.56        44\n",
      "           3       0.67      0.14      0.24        14\n",
      "           4       0.72      0.62      0.67        34\n",
      "           5       0.69      0.67      0.68        72\n",
      "           6       0.52      0.65      0.57        48\n",
      "\n",
      "    accuracy                           0.69       452\n",
      "   macro avg       0.67      0.60      0.60       452\n",
      "weighted avg       0.70      0.69      0.69       452\n",
      "\n",
      "fold number=  9\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 52ms/step - loss: 1.7933 - val_loss: 1.5164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.51643, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.3980 - val_loss: 1.3910\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.51643 to 1.39096, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.2790 - val_loss: 1.3237\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.39096 to 1.32370, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.1751 - val_loss: 1.2629\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32370 to 1.26293, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0951 - val_loss: 1.2427\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.26293 to 1.24271, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0245 - val_loss: 1.2015\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24271 to 1.20154, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9740 - val_loss: 1.1748\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.20154 to 1.17483, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.9281 - val_loss: 1.1586\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.17483 to 1.15859, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8643 - val_loss: 1.1472\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.15859 to 1.14719, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.8276 - val_loss: 1.1500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.14719\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7754 - val_loss: 1.1301\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.14719 to 1.13007, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7478 - val_loss: 1.1131\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.13007 to 1.11308, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7070 - val_loss: 1.0895\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.11308 to 1.08952, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6716 - val_loss: 1.0498\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.08952 to 1.04980, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6497 - val_loss: 1.0594TA: 0s - l\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.04980\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6099 - val_loss: 1.1175\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.04980\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6029 - val_loss: 1.1097\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.04980\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 29ms/step - loss: 0.5789 - val_loss: 1.0871\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.04980\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5421 - val_loss: 1.0995\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.04980\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5231 - val_loss: 1.1481\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.04980\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5076 - val_loss: 1.1416\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.04980\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5043 - val_loss: 1.1322\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.04980\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4853 - val_loss: 1.0944\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.04980\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 28ms/step - loss: 0.4799 - val_loss: 1.1091\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.04980\n",
      "Epoch 00024: early stopping\n",
      "[[115  11   3   0   2   8   3]\n",
      " [ 13  73   3   1   4   2   2]\n",
      " [  8  10  18   0   3   3   2]\n",
      " [  4   2   1   5   0   0   2]\n",
      " [  7   4   1   0  17   3   2]\n",
      " [ 13   8   0   0   2  47   2]\n",
      " [  6   3   0   0   4   7  28]]\n",
      "Accuracy:67.04%\n",
      "Precision:67.04%\n",
      "Recall:67.04%\n",
      "F1 score:67.04%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.81      0.75       142\n",
      "           1       0.66      0.74      0.70        98\n",
      "           2       0.69      0.41      0.51        44\n",
      "           3       0.83      0.36      0.50        14\n",
      "           4       0.53      0.50      0.52        34\n",
      "           5       0.67      0.65      0.66        72\n",
      "           6       0.68      0.58      0.63        48\n",
      "\n",
      "    accuracy                           0.67       452\n",
      "   macro avg       0.68      0.58      0.61       452\n",
      "weighted avg       0.67      0.67      0.66       452\n",
      "\n",
      "fold number=  10\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 100)         763000    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 874,157\n",
      "Trainable params: 111,057\n",
      "Non-trainable params: 763,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 56ms/step - loss: 1.7581 - val_loss: 1.3997\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.39966, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.4059 - val_loss: 1.2190\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.39966 to 1.21902, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.2683 - val_loss: 1.1444\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.21902 to 1.14439, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.1714 - val_loss: 1.0968\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.14439 to 1.09675, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 1.0861 - val_loss: 1.0510\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.09675 to 1.05104, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 1.0294 - val_loss: 1.0249\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.05104 to 1.02493, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.9708 - val_loss: 0.9789\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.02493 to 0.97891, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.9259 - val_loss: 1.0126\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.97891\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8665 - val_loss: 1.0115\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.97891\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.8196 - val_loss: 1.0174\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.97891\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.7700 - val_loss: 1.0097\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.97891\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.7318 - val_loss: 0.9727\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.97891 to 0.97268, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6976 - val_loss: 0.9733\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.97268\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.6763 - val_loss: 0.9890\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.97268\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6350 - val_loss: 1.0106\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.97268\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.6115 - val_loss: 1.0369\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.97268\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5718 - val_loss: 1.0413\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.97268\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5664 - val_loss: 1.0517\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.97268\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.5491 - val_loss: 1.0577\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.97268\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5094 - val_loss: 1.0242\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.97268\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4935 - val_loss: 1.0303\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.97268\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4828 - val_loss: 1.0361\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.97268\n",
      "Epoch 00022: early stopping\n",
      "[[118   5   4   0   1   2  12]\n",
      " [ 15  67   5   1   1   3   6]\n",
      " [  6   2  28   0   2   4   2]\n",
      " [  3   3   0   4   1   1   2]\n",
      " [  4   4   4   0  16   1   5]\n",
      " [ 11   6   4   1   2  44   4]\n",
      " [  5   4   2   1   4   0  32]]\n",
      "Accuracy:68.36%\n",
      "Precision:68.36%\n",
      "Recall:68.36%\n",
      "F1 score:68.36%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78       142\n",
      "           1       0.74      0.68      0.71        98\n",
      "           2       0.60      0.64      0.62        44\n",
      "           3       0.57      0.29      0.38        14\n",
      "           4       0.59      0.47      0.52        34\n",
      "           5       0.80      0.61      0.69        72\n",
      "           6       0.51      0.67      0.58        48\n",
      "\n",
      "    accuracy                           0.68       452\n",
      "   macro avg       0.65      0.60      0.61       452\n",
      "weighted avg       0.69      0.68      0.68       452\n",
      "\n",
      "Cross Validation is completed after 557123\n",
      "accuracy: 69.84% (2.89%)\n",
      "precision: 69.84% (2.89%)\n",
      "recall: 69.84% (2.89%)\n",
      "f1: 69.84% (2.89%)\n"
     ]
    }
   ],
   "source": [
    "if multi == True and representation_form == \"sequences\" and train_w2v == False:\n",
    "    runDLCrossVal_multi(lines_pad, data[\"Category_Index\"].values, 512, num_words, dim, seed, embedding_matrix, \"lstm\", multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b9aa3",
   "metadata": {},
   "source": [
    "One more option for training your own word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55864532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWord2Vec(tokenized_list, max_length, dim):\n",
    "    # this should be executed only in the training set during cross-validation\n",
    "    w2v_model = Word2Vec(sentences=tokenized_list, vector_size=dim, window=5, min_count=1, workers=4, epochs=4)\n",
    "    #w2v_model.save(\"python_word2vec.model\")\n",
    "    #w2v_model = Word2Vec.load(\"python_word2vec.model\")\n",
    "\n",
    "    #     fileEmb = 'w2v_embeddingsIters.txt'\n",
    "    #     w2v_vectors.save_word2vec_format(fileEmb, binary=False)\n",
    "\n",
    "    #     embeddings_index = {}\n",
    "    #     f = open(os.path.join('', fileEmb), encoding=\"utf-8\")\n",
    "    #     for line in f:    \n",
    "    #         values = line.split()\n",
    "    #         word = values[0]\n",
    "    #         coefs = np.asarray(values[1:])\n",
    "    #         embeddings_index[word] = coefs   \n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "\n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'w2v_new_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "\n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "\n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "\n",
    "\n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = w2v_model.wv[word] if word in w2v_model.wv else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return lines_pad, embedding_matrix, num_words, tokenizerFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e99615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWord2Vec(tokenized_list, tokenizer_path, max_length):\n",
    "    \n",
    "    with open(tokenizer_path) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "    \n",
    "    return lines_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "275db8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDLCrossValW2v_multi(X, y, max_len, dim, seed, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train, embedding_matrix, num_words, tokenizerFile = trainWord2Vec(X_train, max_len, dim)\n",
    "        \n",
    "        X_test = testWord2Vec(X_test, tokenizerFile, max_len)\n",
    "        \n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        \n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = np.argmax(predScores, axis=1)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='macro')\n",
    "        recall=recall_score(Y_test, predictions, average='macro')\n",
    "        f1=f1_score(Y_test, predictions, average='macro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de802e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_w2v == True and multi == True and representation_form == \"sequences\":\n",
    "    dim = 300\n",
    "    runDLCrossValW2v_multi(np.array(tokenized_list), data[\"Category_Index\"].values, 512, dim, seed, \"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4504cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
