{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader\n",
    "import io\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e24c8",
   "metadata": {},
   "source": [
    "Set the seeder to have as stable random operations as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a6448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_algorithms = [\"w2v\", \"ft\", \"ft_code\", \"bert\", \"codebert\"]\n",
    "embedding_algorithm = embedding_algorithms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f9b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"bow\", \"sequences\"]\n",
    "representation_form = representations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3acc7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary or multi-class\n",
    "multi = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a56a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2010727",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if representation_form == \"bow\":\n",
    "    data = pd.read_csv('bow_data.csv') # bow\n",
    "else:\n",
    "    data = pd.read_csv('sequences_data.csv') # sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abc9724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vulnerability</th>\n",
       "      <th>Category</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f\"str$id\"\"str$id\"\"str$id\"         ...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client.listentcp()    proxy = proxy(proxy_...</td>\n",
       "      <td>xsrf</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from django.http import httpresponse, httpresp...</td>\n",
       "      <td>open_redirect</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def write_preset(conn, queryin, descriptin):\\t...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>update_query = self.up...</td>\n",
       "      <td>sql_injection</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Vulnerability       Category  Length\n",
       "0              f\"str$id\"\"str$id\"\"str$id\"         ...  sql_injection       9\n",
       "1      client.listentcp()    proxy = proxy(proxy_...           xsrf       8\n",
       "2  from django.http import httpresponse, httpresp...  open_redirect       9\n",
       "3  def write_preset(conn, queryin, descriptin):\\t...  sql_injection     175\n",
       "4                          update_query = self.up...  sql_injection      14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5d5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if representation_form == \"bow\":\n",
    "    bow_size = 237 # number of columns that stand as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e46cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48846336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 392\n"
     ]
    }
   ],
   "source": [
    "if representation_form == \"sequences\":\n",
    "    word_counts = data[\"Vulnerability\"].apply(lambda x: len(x.split()))\n",
    "    max_length = word_counts.max()\n",
    "    print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bbacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Frequencies:\n",
      " sql_injection            1424\n",
      "xsrf                      976\n",
      "command_injection         721\n",
      "path_disclosure           481\n",
      "open_redirect             442\n",
      "remote_code_execution     334\n",
      "xss                       145\n",
      "Name: Category, dtype: int64\n",
      "Total samples  4523\n"
     ]
    }
   ],
   "source": [
    "label_frequencies = data['Category'].value_counts()\n",
    "print(\"Label Frequencies:\\n\", label_frequencies)\n",
    "print(\"Total samples \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b777af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True:\n",
    "    n_categories = len(label_frequencies) # 7\n",
    "    n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d18f",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acbd1",
   "metadata": {},
   "source": [
    "Word2Vec - load pre-trained word2vec embeddings - NL knowledge - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0d6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaskaloup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if representation_form == \"sequences\":\n",
    "\n",
    "    # Download the Punkt tokenizer models if not already downloaded\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    tokenized_list = [word_tokenize(sentence) for sentence in data[\"Vulnerability\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0f0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"w2v\" and representation_form == \"sequences\" and train_w2v == False:\n",
    "\n",
    "    w2v_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "    dim = w2v_vectors.vector_size\n",
    "\n",
    "#     fileEmb = 'w2v_embeddingsIters.txt'\n",
    "#     w2v_vectors.save_word2vec_format(fileEmb, binary=False)\n",
    "    \n",
    "#     embeddings_index = {}\n",
    "#     f = open(os.path.join('', fileEmb), encoding=\"utf-8\")\n",
    "#     for line in f:    \n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:])\n",
    "#         embeddings_index[word] = coefs   \n",
    "#     f.close()\n",
    "    \n",
    "    \n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "    \n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'w2v_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "    \n",
    "    \n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = w2v_vectors[word] if word in w2v_vectors else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f8c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"ft\" and representation_form == \"sequences\":\n",
    "\n",
    "    fastText_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "    dim = fastText_vectors.vector_size   \n",
    "    \n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "    \n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'fast_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)    \n",
    "    \n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = fastText_vectors[word] if word in fastText_vectors else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d71bf",
   "metadata": {},
   "source": [
    "Pre-trained FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"ft_code\" and representation_form == \"sequences\":\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('', 'fast-text_embeddings.txt'), encoding=\"utf-8\")\n",
    "    for line in f:    \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:])\n",
    "        embeddings_index[word] = coefs   \n",
    "    f.close() \n",
    "    \n",
    "    dim = 100\n",
    "    \n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "    \n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'fastCode_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)    \n",
    "    \n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9edbb",
   "metadata": {},
   "source": [
    "BERT - load pre-trained bert embeddings - NL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b99ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"bert\" and representation_form == \"sequences\": \n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)\n",
    "    \n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c734b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"bert\" and representation_form == \"sequences\":\n",
    "    sentences = data[\"Vulnerability\"].tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "    \n",
    "    lines_pad = []\n",
    "    for seq in sequences:\n",
    "        lines_pad.append(seq[0])\n",
    "    \n",
    "    lines_pad = pad_sequences(lines_pad, padding = 'post', maxlen = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41a30e",
   "metadata": {},
   "source": [
    "CodeBERT - load pre-trained codebert embeddings - PL knowledge - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8be0cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"codebert\" and representation_form == \"sequences\": \n",
    "    model_variation = \"microsoft/codebert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation)\n",
    "    \n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f596d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02135a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_algorithm == \"codebert\" and representation_form == \"sequences\": \n",
    "\n",
    "    sentences = data[\"Vulnerability\"].tolist()\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]\n",
    "    \n",
    "    lines_pad = padSequences(sequences, 512)\n",
    "    lines_pad = [arr.tolist() for arr in lines_pad]\n",
    "    lines_pad = np.array(lines_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ce97",
   "metadata": {},
   "source": [
    "RNN model, LSTM specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7222be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, multi, n_outputs):\n",
    "    model=Sequential()\n",
    "    #model.add(Embedding(input_dim=top_words+1, output_dim=dim, input_length=None, mask_zero=True))\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(50, dropout=0.1, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    model.add(BatchNormalization(momentum=0.0))\n",
    "    if multi == False:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')  \n",
    "    else: \n",
    "        model.add(Dense(n_outputs, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f9769",
   "metadata": {},
   "source": [
    "CNN model 1-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "688dbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix, multi, n_outputs):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 1, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    if multi == False:\n",
    "        cnn_model.add(Dense(n_outputs, activation = 'sigmoid'))\n",
    "        cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\")\n",
    "    else: \n",
    "        model.add(Dense(n_outputs, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab4fc2",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd95cd0",
   "metadata": {},
   "source": [
    "Binary Classification: Recognition of Injection Vulnerabilities (command_injection and sql_injection merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab9f8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if the category is an injection or not\n",
    "def is_injection(category):\n",
    "    if category in ['sql_injection', 'command_injection']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "if multi == False:\n",
    "    data['Injection'] = data['Category'].apply(is_injection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcdff955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW and ML\n",
    "\n",
    "def runMLCrossVal_binary(X, y, seed, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"svm\":\n",
    "            myModel = SVC(kernel='rbf', gamma=100)\n",
    "        elif userModel == \"RF\":\n",
    "            myModel = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt')\n",
    "        elif userModel == \"DT\":\n",
    "            myModel = tree.DecisionTreeClassifier(max_depth=120)\n",
    "        elif userModel == \"NB\":\n",
    "            myModel = GaussianNB()\n",
    "\n",
    "        myModel.fit(X_train, Y_train.ravel())\n",
    "        \n",
    "        predictions = myModel.predict(X_test)\n",
    "        #predScores = myModel.predict_proba(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions)\n",
    "        recall=recall_score(Y_test, predictions)\n",
    "        f1=f1_score(Y_test, predictions)\n",
    "        roc_auc=roc_auc_score(Y_test, predictions)\n",
    "        print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "    print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4da8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == False and representation_form == \"bow\":\n",
    "    runMLCrossVal_binary(data.iloc[:, 0:bow_size], data[\"Injection\"], seed, \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "246db4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences of tokens and DL\n",
    "\n",
    "def runDLCrossVal_binary(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel, multi):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, multi, 1) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, multi, 1)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predictions = (myModel.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        predScores = myModel.predict(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions)\n",
    "        recall=recall_score(Y_test, predictions)\n",
    "        f1=f1_score(Y_test, predictions)\n",
    "        roc_auc=roc_auc_score(Y_test, predictions)\n",
    "        print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "    print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ced8e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == False and representation_form == \"sequences\":\n",
    "    runDLCrossVal_binary(lines_pad, data[\"Injection\"].values, 512, num_words, dim, seed, embedding_matrix, \"lstm\", multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3dea",
   "metadata": {},
   "source": [
    "Multi-class Classification: Categorization of all detected vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab8dd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True:\n",
    "    \n",
    "    # Convert categories to numerical indexes\n",
    "    category_numerical_indexes, unique_categories = data[\"Category\"].factorize()\n",
    "\n",
    "    # Create a dictionary mapping each category to its numerical index\n",
    "    category_to_index = {category: index for index, category in enumerate(unique_categories)}\n",
    "\n",
    "    # Update the categories in the DataFrame with their numerical indexes\n",
    "    data[\"Category_Index\"] = data[\"Category\"].map(category_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e8cf6",
   "metadata": {},
   "source": [
    "Use ML models and BoW code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "613fd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMLCrossVal_multi(X, y, seed, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"svm\":\n",
    "            myModel = SVC(kernel='rbf', gamma=100)\n",
    "        elif userModel == \"RF\":\n",
    "            myModel = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt')\n",
    "        elif userModel == \"DT\":\n",
    "            myModel = tree.DecisionTreeClassifier(max_depth=120)\n",
    "        elif userModel == \"NB\":\n",
    "            myModel = GaussianNB()\n",
    "            \n",
    "        myModel.fit(X_train, Y_train.ravel())\n",
    "        \n",
    "        predictions = myModel.predict(X_test)\n",
    "        #predScores = myModel.predict_proba(X_test)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='micro')\n",
    "        recall=recall_score(Y_test, predictions, average='micro')\n",
    "        f1=f1_score(Y_test, predictions, average='micro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        #tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "        #acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8488cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi == True and representation_form == \"bow\":\n",
    "    runMLCrossVal_multi(data.iloc[:, 0:bow_size], data[\"Category_Index\"], seed, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28251b1",
   "metadata": {},
   "source": [
    "Use DL models and sequences of tokens code representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "881e21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDLCrossVal_multi(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel, multi):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = np.argmax(predScores, axis=1)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='micro')\n",
    "        recall=recall_score(Y_test, predictions, average='micro')\n",
    "        f1=f1_score(Y_test, predictions, average='micro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd4ae015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "fold number=  1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 10s 58ms/step - loss: 1.7296 - val_loss: 1.5684\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56837, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.4926 - val_loss: 1.4376\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56837 to 1.43764, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.4072 - val_loss: 1.3737 ETA: 0s - loss: 1.425 - ETA: 0s - loss - ETA: 0s - loss: \n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43764 to 1.37371, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.3419 - val_loss: 1.3305\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37371 to 1.33049, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2923 - val_loss: 1.3183\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33049 to 1.31829, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2556 - val_loss: 1.2864\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.31829 to 1.28638, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2242 - val_loss: 1.2875\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.28638\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1749 - val_loss: 1.2647\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.28638 to 1.26467, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.1546 - val_loss: 1.2653\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.26467\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1147 - val_loss: 1.2588\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.26467 to 1.25876, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0747 - val_loss: 1.2318\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.25876 to 1.23178, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0565 - val_loss: 1.2423\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.23178\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0332 - val_loss: 1.2286TA: 0s - loss: 1.024 - ETA: 0s - loss\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.23178 to 1.22856, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9931 - val_loss: 1.2083\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.22856 to 1.20831, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9584 - val_loss: 1.2000\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.20831 to 1.19997, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9404 - val_loss: 1.1782\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.19997 to 1.17824, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9152 - val_loss: 1.1891\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.17824\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8834 - val_loss: 1.1867\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.17824\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8625 - val_loss: 1.1495ETA: 0s - loss: - ETA: 0s - loss: 0\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.17824 to 1.14948, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8288 - val_loss: 1.1694\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.14948\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8184 - val_loss: 1.2207\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.14948\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7905 - val_loss: 1.1673\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.14948\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7486 - val_loss: 1.1725\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.14948\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7291 - val_loss: 1.1828\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.14948\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7053 - val_loss: 1.2146\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.14948\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6831 - val_loss: 1.2527oss: 0\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.14948\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6725 - val_loss: 1.2479\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.14948\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6406 - val_loss: 1.1864\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.14948\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6244 - val_loss: 1.1474TA: 0s - loss: \n",
      "\n",
      "Epoch 00029: val_loss improved from 1.14948 to 1.14741, saving model to best_model.h5\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6124 - val_loss: 1.1493\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.14741\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 38ms/step - loss: 0.5835 - val_loss: 1.1869\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.14741\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5611 - val_loss: 1.2088\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.14741\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5302 - val_loss: 1.2197\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.14741\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5148 - val_loss: 1.2191 0\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.14741\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5136 - val_loss: 1.1986\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.14741\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4774 - val_loss: 1.2389\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.14741\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4715 - val_loss: 1.2519\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.14741\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4593 - val_loss: 1.3228\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.14741\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4379 - val_loss: 1.3199\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.14741\n",
      "Epoch 00039: early stopping\n",
      "[[117   7   4   0   2   7   6]\n",
      " [  9  67  12   0   2   3   4]\n",
      " [  7   4  23   1   4   2   4]\n",
      " [  4   2   1   6   1   0   0]\n",
      " [  3   4   5   0  18   2   2]\n",
      " [  7   7   2   1   1  51   3]\n",
      " [  4   6   2   1   6   1  28]]\n",
      "Accuracy:68.43%\n",
      "Precision:68.43%\n",
      "Recall:68.43%\n",
      "F1 score:68.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       143\n",
      "           1       0.69      0.69      0.69        97\n",
      "           2       0.47      0.51      0.49        45\n",
      "           3       0.67      0.43      0.52        14\n",
      "           4       0.53      0.53      0.53        34\n",
      "           5       0.77      0.71      0.74        72\n",
      "           6       0.60      0.58      0.59        48\n",
      "\n",
      "    accuracy                           0.68       453\n",
      "   macro avg       0.64      0.61      0.62       453\n",
      "weighted avg       0.69      0.68      0.68       453\n",
      "\n",
      "fold number=  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 58ms/step - loss: 1.7400 - val_loss: 1.5583\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55833, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.5262 - val_loss: 1.4169\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55833 to 1.41689, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4063 - val_loss: 1.3469\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41689 to 1.34693, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3614 - val_loss: 1.3306\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.34693 to 1.33060, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2996 - val_loss: 1.2756\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33060 to 1.27559, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2825 - val_loss: 1.2346\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.27559 to 1.23462, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2132 - val_loss: 1.2014\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.23462 to 1.20138, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1840 - val_loss: 1.1707- ETA: 0\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.20138 to 1.17066, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1499 - val_loss: 1.1615\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.17066 to 1.16150, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1062 - val_loss: 1.1104\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.16150 to 1.11044, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0866 - val_loss: 1.1403\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.11044\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0415 - val_loss: 1.0881 0s - l\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.11044 to 1.08805, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0087 - val_loss: 1.0551\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.08805 to 1.05512, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9790 - val_loss: 1.0646\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.05512\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9452 - val_loss: 1.0530\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.05512 to 1.05301, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9080 - val_loss: 1.0655\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05301\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8823 - val_loss: 1.0496\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.05301 to 1.04955, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8582 - val_loss: 1.0473A: 0s - loss: 0.85\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.04955 to 1.04732, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8432 - val_loss: 1.1149\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.04732\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8140 - val_loss: 1.0124- \n",
      "\n",
      "Epoch 00020: val_loss improved from 1.04732 to 1.01242, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7954 - val_loss: 1.0294\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.01242\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7559 - val_loss: 1.0531\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.01242\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7184 - val_loss: 1.0649 loss: \n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.01242\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7045 - val_loss: 1.1003\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.01242\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6761 - val_loss: 1.0541\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.01242\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6624 - val_loss: 1.0867\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.01242\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6506 - val_loss: 1.1047\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.01242\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6089 - val_loss: 1.1039\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.01242\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5988 - val_loss: 1.0807\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.01242\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5854 - val_loss: 1.1842\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.01242\n",
      "Epoch 00030: early stopping\n",
      "[[112   8  10   2   3   6   2]\n",
      " [ 12  73   7   0   1   3   1]\n",
      " [  7   7  26   0   1   3   1]\n",
      " [  0   0   0   6   2   6   0]\n",
      " [  5   5   5   0  16   0   2]\n",
      " [ 12   3   2   4   0  49   3]\n",
      " [  7   5   4   1   6   5  20]]\n",
      "Accuracy:66.67%\n",
      "Precision:66.67%\n",
      "Recall:66.67%\n",
      "F1 score:66.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75       143\n",
      "           1       0.72      0.75      0.74        97\n",
      "           2       0.48      0.58      0.53        45\n",
      "           3       0.46      0.43      0.44        14\n",
      "           4       0.55      0.48      0.52        33\n",
      "           5       0.68      0.67      0.68        73\n",
      "           6       0.69      0.42      0.52        48\n",
      "\n",
      "    accuracy                           0.67       453\n",
      "   macro avg       0.62      0.59      0.60       453\n",
      "weighted avg       0.67      0.67      0.66       453\n",
      "\n",
      "fold number=  3\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 8s 58ms/step - loss: 1.7231 - val_loss: 1.5852\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.58522, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4857 - val_loss: 1.4900\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.58522 to 1.48999, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3950 - val_loss: 1.4250\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.48999 to 1.42504, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.3172 - val_loss: 1.4291\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.42504\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2617 - val_loss: 1.3896\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.42504 to 1.38955, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2250 - val_loss: 1.3619\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.38955 to 1.36192, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1874 - val_loss: 1.3362\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.36192 to 1.33624, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1329 - val_loss: 1.3270\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.33624 to 1.32699, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1002 - val_loss: 1.3152\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.32699 to 1.31520, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0595 - val_loss: 1.2833\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.31520 to 1.28327, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0372 - val_loss: 1.2793\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.28327 to 1.27931, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0060 - val_loss: 1.2791\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.27931 to 1.27911, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9901 - val_loss: 1.2968ETA: 0s - loss: \n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.27911\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9495 - val_loss: 1.3452\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.27911\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9119 - val_loss: 1.2796\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.27911\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8784 - val_loss: 1.2728\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.27911 to 1.27284, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8493 - val_loss: 1.2849\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.27284\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8255 - val_loss: 1.3753: 0.82\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.27284\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8097 - val_loss: 1.2624 0s -\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.27284 to 1.26244, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7749 - val_loss: 1.2596\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.26244 to 1.25959, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7350 - val_loss: 1.1961\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.25959 to 1.19611, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7168 - val_loss: 1.2227\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.19611\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6844 - val_loss: 1.2264\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.19611\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6591 - val_loss: 1.2532\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.19611\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6278 - val_loss: 1.2564\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.19611\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6094 - val_loss: 1.2279os\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.19611\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5986 - val_loss: 1.2223 0.59\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.19611\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5602 - val_loss: 1.2447\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.19611\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5462 - val_loss: 1.2798\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.19611\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5256 - val_loss: 1.2463\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.19611\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.505 - 2s 33ms/step - loss: 0.5057 - val_loss: 1.2821\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.19611\n",
      "Epoch 00031: early stopping\n",
      "[[104  10   7   0   5  11   6]\n",
      " [ 13  76   4   0   0   1   3]\n",
      " [ 11   5  19   2   3   4   0]\n",
      " [  4   3   1   4   2   1   0]\n",
      " [  6   4   5   0  15   2   1]\n",
      " [ 10   5   6   0   3  44   4]\n",
      " [ 13   4   1   1   3   6  21]]\n",
      "Accuracy:62.47%\n",
      "Precision:62.47%\n",
      "Recall:62.47%\n",
      "F1 score:62.47%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.73      0.68       143\n",
      "           1       0.71      0.78      0.75        97\n",
      "           2       0.44      0.43      0.44        44\n",
      "           3       0.57      0.27      0.36        15\n",
      "           4       0.48      0.45      0.47        33\n",
      "           5       0.64      0.61      0.62        72\n",
      "           6       0.60      0.43      0.50        49\n",
      "\n",
      "    accuracy                           0.62       453\n",
      "   macro avg       0.58      0.53      0.55       453\n",
      "weighted avg       0.62      0.62      0.62       453\n",
      "\n",
      "fold number=  4\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 58ms/step - loss: 1.7151 - val_loss: 1.5378\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53785, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.5095 - val_loss: 1.4403\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53785 to 1.44032, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.4166 - val_loss: 1.3609\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44032 to 1.36085, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.3583 - val_loss: 1.3368\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36085 to 1.33684, saving model to best_model.h5\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 1.3050 - val_loss: 1.3214\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33684 to 1.32142, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2620 - val_loss: 1.2603\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.32142 to 1.26030, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2337 - val_loss: 1.2290\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26030 to 1.22895, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1756 - val_loss: 1.2295\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.22895\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1519 - val_loss: 1.2406\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.22895\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1205 - val_loss: 1.1859\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.22895 to 1.18594, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0975 - val_loss: 1.2115\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.18594\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0717 - val_loss: 1.1748\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.18594 to 1.17484, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0361 - val_loss: 1.1361\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.17484 to 1.13608, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0107 - val_loss: 1.1095\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.13608 to 1.10950, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9657 - val_loss: 1.1096\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.10950\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.9345 - val_loss: 1.0754\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.10950 to 1.07542, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9148 - val_loss: 1.0617\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.07542 to 1.06171, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8897 - val_loss: 1.0440\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.06171 to 1.04400, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8671 - val_loss: 1.0265\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.04400 to 1.02649, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.8271 - val_loss: 1.02450s\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.02649 to 1.02448, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8006 - val_loss: 1.0603\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.02448\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7882 - val_loss: 1.0814\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.02448\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7604 - val_loss: 1.0214\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.02448 to 1.02142, saving model to best_model.h5\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7281 - val_loss: 1.0408\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.02142\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6962 - val_loss: 1.0692\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.02142\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6807 - val_loss: 1.0646\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.02142\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6819 - val_loss: 1.0586\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.02142\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6472 - val_loss: 1.0757\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.02142\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6378 - val_loss: 1.0657- ETA: 0s - loss: 0.636\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.02142\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6145 - val_loss: 1.0603\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.02142\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5935 - val_loss: 1.1004\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.02142\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5666 - val_loss: 1.0902\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.02142\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5517 - val_loss: 1.0640\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.02142\n",
      "Epoch 00033: early stopping\n",
      "[[116   4   4   1   2  13   3]\n",
      " [ 15  67   8   1   2   2   2]\n",
      " [ 10   2  22   0   0   2   8]\n",
      " [  4   2   4   2   0   2   1]\n",
      " [ 10   4   2   1  13   0   3]\n",
      " [  8   5   4   2   2  50   1]\n",
      " [ 17   0   5   0   1   3  22]]\n",
      "Accuracy:64.60%\n",
      "Precision:64.60%\n",
      "Recall:64.60%\n",
      "F1 score:64.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.72       143\n",
      "           1       0.80      0.69      0.74        97\n",
      "           2       0.45      0.50      0.47        44\n",
      "           3       0.29      0.13      0.18        15\n",
      "           4       0.65      0.39      0.49        33\n",
      "           5       0.69      0.69      0.69        72\n",
      "           6       0.55      0.46      0.50        48\n",
      "\n",
      "    accuracy                           0.65       452\n",
      "   macro avg       0.58      0.53      0.54       452\n",
      "weighted avg       0.64      0.65      0.64       452\n",
      "\n",
      "fold number=  5\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 57ms/step - loss: 1.7347 - val_loss: 1.5776\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.57764, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4899 - val_loss: 1.4661\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.57764 to 1.46607, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4057 - val_loss: 1.3956 E\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46607 to 1.39563, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3460 - val_loss: 1.3403\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.39563 to 1.34034, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2868 - val_loss: 1.29540s - loss: 1\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34034 to 1.29544, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2522 - val_loss: 1.2693\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29544 to 1.26931, saving model to best_model.h5\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2014 - val_loss: 1.2598\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26931 to 1.25977, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1828 - val_loss: 1.2582\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.25977 to 1.25820, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1494 - val_loss: 1.2317\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.25820 to 1.23171, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1247 - val_loss: 1.2139\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.23171 to 1.21393, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0747 - val_loss: 1.2256\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.21393\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0483 - val_loss: 1.1905TA: 0s - \n",
      "\n",
      "Epoch 00012: val_loss improved from 1.21393 to 1.19047, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0119 - val_loss: 1.1672\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.19047 to 1.16721, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9862 - val_loss: 1.1527: 0s - loss\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.16721 to 1.15270, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9480 - val_loss: 1.1425\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.15270 to 1.14254, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9195 - val_loss: 1.15610s -\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.14254\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9061 - val_loss: 1.1232\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.14254 to 1.12319, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8972 - val_loss: 1.1082\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.12319 to 1.10816, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8425 - val_loss: 1.1310\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.10816\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8046 - val_loss: 1.1284\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.10816\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7819 - val_loss: 1.1651\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.10816\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7680 - val_loss: 1.1136\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.10816\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7473 - val_loss: 1.1163\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.10816\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7198 - val_loss: 1.0862\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.10816 to 1.08616, saving model to best_model.h5\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7124 - val_loss: 1.1333\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.08616\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6795 - val_loss: 1.1455\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.08616\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6509 - val_loss: 1.1352\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.08616\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6391 - val_loss: 1.0929\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.08616\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6163 - val_loss: 1.1093\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.08616\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5878 - val_loss: 1.1012\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.08616\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5867 - val_loss: 1.1147\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.08616\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5687 - val_loss: 1.1003\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.08616\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5384 - val_loss: 1.1056\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.08616\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5124 - val_loss: 1.1089\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.08616\n",
      "Epoch 00034: early stopping\n",
      "[[108   6   5   2   3  13   5]\n",
      " [  8  72   7   1   1   6   3]\n",
      " [ 12   6  17   1   4   2   2]\n",
      " [  1   1   2   2   1   6   2]\n",
      " [  0   4   1   1  21   4   2]\n",
      " [  5   6   0   0   2  55   4]\n",
      " [  5   4   1   0   2   9  27]]\n",
      "Accuracy:66.81%\n",
      "Precision:66.81%\n",
      "Recall:66.81%\n",
      "F1 score:66.81%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       142\n",
      "           1       0.73      0.73      0.73        98\n",
      "           2       0.52      0.39      0.44        44\n",
      "           3       0.29      0.13      0.18        15\n",
      "           4       0.62      0.64      0.63        33\n",
      "           5       0.58      0.76      0.66        72\n",
      "           6       0.60      0.56      0.58        48\n",
      "\n",
      "    accuracy                           0.67       452\n",
      "   macro avg       0.59      0.57      0.57       452\n",
      "weighted avg       0.66      0.67      0.66       452\n",
      "\n",
      "fold number=  6\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 57ms/step - loss: 1.7280 - val_loss: 1.5101\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.51007, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.5003 - val_loss: 1.4262\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.51007 to 1.42617, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4107 - val_loss: 1.3639\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42617 to 1.36389, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3522 - val_loss: 1.3486\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36389 to 1.34863, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2896 - val_loss: 1.3116\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34863 to 1.31160, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2352 - val_loss: 1.2690l\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.31160 to 1.26904, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1956 - val_loss: 1.2669\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26904 to 1.26685, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1471 - val_loss: 1.2653\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.26685 to 1.26531, saving model to best_model.h5\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1219 - val_loss: 1.2402\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.26531 to 1.24024, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0996 - val_loss: 1.2490\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.24024\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0589 - val_loss: 1.2440\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.24024\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0153 - val_loss: 1.2111\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.24024 to 1.21107, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9709 - val_loss: 1.1967\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.21107 to 1.19673, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9607 - val_loss: 1.1915\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.19673 to 1.19149, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9222 - val_loss: 1.1600s: \n",
      "\n",
      "Epoch 00015: val_loss improved from 1.19149 to 1.15999, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8773 - val_loss: 1.1663\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.15999\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8563 - val_loss: 1.1553\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.15999 to 1.15532, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8211 - val_loss: 1.1989\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.15532\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8049 - val_loss: 1.1285\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.15532 to 1.12848, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7711 - val_loss: 1.2290\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.12848\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7324 - val_loss: 1.1759\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.12848\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7118 - val_loss: 1.2255\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.12848\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6833 - val_loss: 1.1959\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.12848\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6487 - val_loss: 1.2133\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.12848\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6384 - val_loss: 1.1325\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.12848\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6042 - val_loss: 1.1677\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.12848\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5899 - val_loss: 1.1708\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.12848\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5626 - val_loss: 1.1915.\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.12848\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5473 - val_loss: 1.1846\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.12848\n",
      "Epoch 00029: early stopping\n",
      "[[111   6   6   2   4   7   6]\n",
      " [ 14  67   9   1   3   2   2]\n",
      " [ 14   4  16   0   5   4   1]\n",
      " [  5   0   1   5   3   1   0]\n",
      " [  5   7   5   0  14   2   0]\n",
      " [  5   6   1   2   2  54   2]\n",
      " [ 14   7   0   1   2   4  20]]\n",
      "Accuracy:63.50%\n",
      "Precision:63.50%\n",
      "Recall:63.50%\n",
      "F1 score:63.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72       142\n",
      "           1       0.69      0.68      0.69        98\n",
      "           2       0.42      0.36      0.39        44\n",
      "           3       0.45      0.33      0.38        15\n",
      "           4       0.42      0.42      0.42        33\n",
      "           5       0.73      0.75      0.74        72\n",
      "           6       0.65      0.42      0.51        48\n",
      "\n",
      "    accuracy                           0.63       452\n",
      "   macro avg       0.58      0.54      0.55       452\n",
      "weighted avg       0.63      0.63      0.63       452\n",
      "\n",
      "fold number=  7\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 64ms/step - loss: 1.7363 - val_loss: 1.6204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62045, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.5034 - val_loss: 1.5227\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62045 to 1.52268, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3896 - val_loss: 1.4354\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.52268 to 1.43544, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3372 - val_loss: 1.3897TA: 0s -\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.43544 to 1.38973, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2887 - val_loss: 1.3438\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.38973 to 1.34378, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2535 - val_loss: 1.3240\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.34378 to 1.32403, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.2115 - val_loss: 1.2765\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.32403 to 1.27650, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1665 - val_loss: 1.2853\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.27650\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1316 - val_loss: 1.2843 - ETA: 0s - loss: 1.1\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.27650\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1010 - val_loss: 1.2507\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.27650 to 1.25067, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0614 - val_loss: 1.2168\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.25067 to 1.21682, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0441 - val_loss: 1.2363\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.21682\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9980 - val_loss: 1.1946\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.21682 to 1.19458, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9829 - val_loss: 1.1719\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.19458 to 1.17195, saving model to best_model.h5\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9355 - val_loss: 1.1511\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.17195 to 1.15110, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9064 - val_loss: 1.1479\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.15110 to 1.14794, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8815 - val_loss: 1.1444\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.14794 to 1.14442, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8637 - val_loss: 1.1038\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.14442 to 1.10382, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8175 - val_loss: 1.0991\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.10382 to 1.09906, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7947 - val_loss: 1.1084\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.09906\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7747 - val_loss: 1.0742\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.09906 to 1.07418, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7395 - val_loss: 1.0761\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.07418\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7217 - val_loss: 1.0756\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.07418\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6865 - val_loss: 1.0573\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.07418 to 1.05731, saving model to best_model.h5\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6890 - val_loss: 1.0994\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.05731\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6540 - val_loss: 1.1250\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.05731\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6255 - val_loss: 1.0487\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.05731 to 1.04865, saving model to best_model.h5\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6376 - val_loss: 1.1373\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.04865\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5966 - val_loss: 1.0867\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.04865\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5715 - val_loss: 1.0475\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.04865 to 1.04752, saving model to best_model.h5\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5484- ETA: 1s - loss: 0 - ETA: 1s  - ETA: 0s - los - 2s 33ms/step - loss: 0.5481 - val_loss: 1.0580\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.04752\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5251 - val_loss: 1.1139\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.04752\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5220 - val_loss: 1.1685\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.04752\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5015 - val_loss: 1.1395s\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.04752\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4854 - val_loss: 1.0807\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.04752\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4602 - val_loss: 1.1729\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.04752\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4411 - val_loss: 1.1255\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.04752\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4466 - val_loss: 1.1526A: 0s - loss: 0 - ETA: 0s - los\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.04752\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4300 - val_loss: 1.1568TA: 1s  - ETA: 0s - loss: 0.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.04752\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4174 - val_loss: 1.1387\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.04752\n",
      "Epoch 00040: early stopping\n",
      "[[111   5   9   1   1   6   9]\n",
      " [  7  66   8   1   5   5   6]\n",
      " [  8   3  26   1   1   0   5]\n",
      " [  3   1   0   7   3   0   1]\n",
      " [  2   2   4   0  19   2   4]\n",
      " [ 12   2   4   0   2  50   2]\n",
      " [  7   1   5   1   3   3  28]]\n",
      "Accuracy:67.92%\n",
      "Precision:67.92%\n",
      "Recall:67.92%\n",
      "F1 score:67.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76       142\n",
      "           1       0.82      0.67      0.74        98\n",
      "           2       0.46      0.59      0.52        44\n",
      "           3       0.64      0.47      0.54        15\n",
      "           4       0.56      0.58      0.57        33\n",
      "           5       0.76      0.69      0.72        72\n",
      "           6       0.51      0.58      0.54        48\n",
      "\n",
      "    accuracy                           0.68       452\n",
      "   macro avg       0.64      0.62      0.63       452\n",
      "weighted avg       0.69      0.68      0.68       452\n",
      "\n",
      "fold number=  8\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 9s 59ms/step - loss: 1.7119 - val_loss: 1.5861\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.58612, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.5006 - val_loss: 1.4591\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.58612 to 1.45911, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.4144 - val_loss: 1.4202\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.45911 to 1.42021, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.3505 - val_loss: 1.3708\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42021 to 1.37082, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2956 - val_loss: 1.32180s - loss: 1.2\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.37082 to 1.32177, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2470 - val_loss: 1.3218\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.32177\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2257 - val_loss: 1.3000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.32177 to 1.29999, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1779 - val_loss: 1.2562\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.29999 to 1.25617, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.1331 - val_loss: 1.2827\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.25617\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0972 - val_loss: 1.2521\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.25617 to 1.25207, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0594 - val_loss: 1.2453\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.25207 to 1.24532, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0270 - val_loss: 1.3084\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.24532\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9963 - val_loss: 1.2920\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.24532\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9603 - val_loss: 1.2322\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.24532 to 1.23223, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9331 - val_loss: 1.2323\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.23223\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9053 - val_loss: 1.2868\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.23223\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8660 - val_loss: 1.2624- l\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.23223\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.827 - 2s 34ms/step - loss: 0.8284 - val_loss: 1.2466\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.23223\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7999 - val_loss: 1.2223\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.23223 to 1.22225, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7836 - val_loss: 1.2143\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.22225 to 1.21427, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7532 - val_loss: 1.1917\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.21427 to 1.19171, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7261 - val_loss: 1.2097 0s - loss: 0.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.19171\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6934 - val_loss: 1.2440\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.19171\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6664 - val_loss: 1.1636\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.19171 to 1.16364, saving model to best_model.h5\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6263 - val_loss: 1.21190\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.16364\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6221 - val_loss: 1.1667\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.16364\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5870 - val_loss: 1.2508\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.16364\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5793 - val_loss: 1.2146\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.16364\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5657 - val_loss: 1.2611\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.16364\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5533 - val_loss: 1.3066ETA: 0s\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.16364\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5302 - val_loss: 1.2691\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.16364\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5030 - val_loss: 1.2676\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.16364\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4845 - val_loss: 1.2835\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.16364\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4615 - val_loss: 1.2301\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.16364\n",
      "Epoch 00034: early stopping\n",
      "[[112   8   4   3   2   4   9]\n",
      " [ 16  66   2   5   3   4   2]\n",
      " [ 14  12  11   0   5   1   1]\n",
      " [  4   0   0   6   0   3   1]\n",
      " [  7   4   1   0  20   0   2]\n",
      " [ 16   8   0   4   0  41   3]\n",
      " [  9   5   0   2   1   6  25]]\n",
      "Accuracy:62.17%\n",
      "Precision:62.17%\n",
      "Recall:62.17%\n",
      "F1 score:62.17%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.79      0.70       142\n",
      "           1       0.64      0.67      0.66        98\n",
      "           2       0.61      0.25      0.35        44\n",
      "           3       0.30      0.43      0.35        14\n",
      "           4       0.65      0.59      0.62        34\n",
      "           5       0.69      0.57      0.63        72\n",
      "           6       0.58      0.52      0.55        48\n",
      "\n",
      "    accuracy                           0.62       452\n",
      "   macro avg       0.59      0.55      0.55       452\n",
      "weighted avg       0.63      0.62      0.61       452\n",
      "\n",
      "fold number=  9\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 9s 58ms/step - loss: 1.7526 - val_loss: 1.5525\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55255, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.5118 - val_loss: 1.4174\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55255 to 1.41735, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4112 - val_loss: 1.3648\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41735 to 1.36479, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.3517 - val_loss: 1.3033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36479 to 1.30332, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.3045 - val_loss: 1.2944\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.30332 to 1.29441, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.2650 - val_loss: 1.2526\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29441 to 1.25261, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2257 - val_loss: 1.2705\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.25261\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1824 - val_loss: 1.2298\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.25261 to 1.22982, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.1346 - val_loss: 1.1877\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.22982 to 1.18769, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1103 - val_loss: 1.1865\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.18769 to 1.18653, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0707 - val_loss: 1.1470\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.18653 to 1.14696, saving model to best_model.h5\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0499 - val_loss: 1.1463\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.14696 to 1.14635, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0126 - val_loss: 1.1675\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.14635\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9829 - val_loss: 1.1435\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.14635 to 1.14351, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9523 - val_loss: 1.1244\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.14351 to 1.12444, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9153 - val_loss: 1.1462\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.12444\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8945 - val_loss: 1.2076 ETA: 0s - l\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.12444\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8688 - val_loss: 1.1475\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.12444\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8439 - val_loss: 1.1864\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.12444\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8207 - val_loss: 1.1418\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.12444\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7743 - val_loss: 1.1245\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.12444\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7477 - val_loss: 1.1107\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.12444 to 1.11072, saving model to best_model.h5\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7259 - val_loss: 1.0948\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.11072 to 1.09482, saving model to best_model.h5\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7159 - val_loss: 1.2891\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.09482\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6905 - val_loss: 1.1505\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.09482\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6621 - val_loss: 1.1006\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.09482\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6247 - val_loss: 1.0979\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.09482\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6065 - val_loss: 1.1434\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.09482\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5856 - val_loss: 1.1225\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.09482\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5460 - val_loss: 1.1782\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.09482\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5235 - val_loss: 1.1258\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.09482\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5093 - val_loss: 1.1715\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.09482\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5221 - val_loss: 1.1980\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.09482\n",
      "Epoch 00033: early stopping\n",
      "[[109  10   4   1   3  12   3]\n",
      " [ 14  72   1   0   3   6   2]\n",
      " [ 13   8  16   1   2   4   0]\n",
      " [  3   2   2   5   0   2   0]\n",
      " [  7   3   1   0  21   1   1]\n",
      " [ 18   0   1   0   0  53   0]\n",
      " [ 18   4   1   0   7   4  14]]\n",
      "Accuracy:64.16%\n",
      "Precision:64.16%\n",
      "Recall:64.16%\n",
      "F1 score:64.16%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.77      0.67       142\n",
      "           1       0.73      0.73      0.73        98\n",
      "           2       0.62      0.36      0.46        44\n",
      "           3       0.71      0.36      0.48        14\n",
      "           4       0.58      0.62      0.60        34\n",
      "           5       0.65      0.74      0.69        72\n",
      "           6       0.70      0.29      0.41        48\n",
      "\n",
      "    accuracy                           0.64       452\n",
      "   macro avg       0.66      0.55      0.58       452\n",
      "weighted avg       0.65      0.64      0.63       452\n",
      "\n",
      "fold number=  10\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 300)         2289000   \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, None, 100)         160400    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 2,480,157\n",
      "Trainable params: 191,057\n",
      "Non-trainable params: 2,289,100\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 8s 63ms/step - loss: 1.7535 - val_loss: 1.4961\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.49608, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.5168 - val_loss: 1.4288 - loss: 1.516\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.49608 to 1.42877, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.4290 - val_loss: 1.3256\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42877 to 1.32559, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3552 - val_loss: 1.2834\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32559 to 1.28343, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.3131 - val_loss: 1.2474\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.28343 to 1.24740, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2671 - val_loss: 1.2326\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24740 to 1.23263, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.2395 - val_loss: 1.2001\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.23263 to 1.20006, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1852 - val_loss: 1.1815\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.20006 to 1.18154, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1522 - val_loss: 1.1539\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.18154 to 1.15393, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.1333 - val_loss: 1.1250\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.15393 to 1.12505, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0783 - val_loss: 1.1466\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.12505\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0525 - val_loss: 1.1008\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.12505 to 1.10076, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0303 - val_loss: 1.1252- loss: 1.025 - ETA: 0s\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.10076\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9856 - val_loss: 1.1046\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.10076\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9654 - val_loss: 1.0519\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.10076 to 1.05192, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9190 - val_loss: 1.1038\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05192\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8923 - val_loss: 1.0941\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05192\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8605 - val_loss: 1.0502\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.05192 to 1.05024, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8308 - val_loss: 1.0798\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05024\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8257 - val_loss: 1.1095s -\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.05024\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7903 - val_loss: 1.1046\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.05024\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7794 - val_loss: 1.1031\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.05024\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7286 - val_loss: 1.0735\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.05024\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7255 - val_loss: 1.0712\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.05024\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6805 - val_loss: 0.9989\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.05024 to 0.99895, saving model to best_model.h5\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6676 - val_loss: 0.9836\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.99895 to 0.98364, saving model to best_model.h5\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6320 - val_loss: 1.0062\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.98364\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6048 - val_loss: 0.9884\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.98364\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5929 - val_loss: 1.0004\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.98364\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5716 - val_loss: 0.9906\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.98364\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5539 - val_loss: 0.9820\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.98364 to 0.98197, saving model to best_model.h5\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5211 - val_loss: 1.0027\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.98197\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4960 - val_loss: 1.0043\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.98197\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4902 - val_loss: 1.0355\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.98197\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4628 - val_loss: 1.0373\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.98197\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4530 - val_loss: 1.0455\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.98197\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4278 - val_loss: 1.0391\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.98197\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4420 - val_loss: 1.0513\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.98197\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4405 - val_loss: 0.9979\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.98197\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.3994 - val_loss: 1.0621\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.98197\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.3902 - val_loss: 1.0753\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.98197\n",
      "Epoch 00041: early stopping\n",
      "[[112   3   6   2   2   9   8]\n",
      " [ 15  69   6   2   2   4   0]\n",
      " [  9   2  27   0   4   2   0]\n",
      " [  4   0   0   7   2   1   0]\n",
      " [  8   3   4   0  17   0   2]\n",
      " [ 10   1   2   1   2  54   2]\n",
      " [  5   3   3   1   3   3  30]]\n",
      "Accuracy:69.91%\n",
      "Precision:69.91%\n",
      "Recall:69.91%\n",
      "F1 score:69.91%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.73       142\n",
      "           1       0.85      0.70      0.77        98\n",
      "           2       0.56      0.61      0.59        44\n",
      "           3       0.54      0.50      0.52        14\n",
      "           4       0.53      0.50      0.52        34\n",
      "           5       0.74      0.75      0.74        72\n",
      "           6       0.71      0.62      0.67        48\n",
      "\n",
      "    accuracy                           0.70       452\n",
      "   macro avg       0.66      0.64      0.65       452\n",
      "weighted avg       0.71      0.70      0.70       452\n",
      "\n",
      "Cross Validation is completed after 838554\n",
      "accuracy: 65.66% (2.52%)\n",
      "precision: 65.66% (2.52%)\n",
      "recall: 65.66% (2.52%)\n",
      "f1: 65.66% (2.52%)\n"
     ]
    }
   ],
   "source": [
    "if multi == True and representation_form == \"sequences\" and train_w2v == False:\n",
    "    runDLCrossVal_multi(lines_pad, data[\"Category_Index\"].values, 512, num_words, dim, seed, embedding_matrix, \"lstm\", multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b9aa3",
   "metadata": {},
   "source": [
    "One more option for training your own word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55864532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWord2Vec(tokenized_list, max_length, dim):\n",
    "    # this should be executed only in the training set during cross-validation\n",
    "    w2v_model = Word2Vec(sentences=tokenized_list, vector_size=dim, window=5, min_count=1, workers=4, epochs=4)\n",
    "    #w2v_model.save(\"python_word2vec.model\")\n",
    "    #w2v_model = Word2Vec.load(\"python_word2vec.model\")\n",
    "\n",
    "    #     fileEmb = 'w2v_embeddingsIters.txt'\n",
    "    #     w2v_vectors.save_word2vec_format(fileEmb, binary=False)\n",
    "\n",
    "    #     embeddings_index = {}\n",
    "    #     f = open(os.path.join('', fileEmb), encoding=\"utf-8\")\n",
    "    #     for line in f:    \n",
    "    #         values = line.split()\n",
    "    #         word = values[0]\n",
    "    #         coefs = np.asarray(values[1:])\n",
    "    #         embeddings_index[word] = coefs   \n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    tokenizer_obj = Tokenizer()   \n",
    "    tokenizer_obj.fit_on_texts(tokenized_list)\n",
    "\n",
    "    tokenizer_json = tokenizer_obj.to_json()\n",
    "    tokenizerFile = 'w2v_new_tokenizer.json'\n",
    "    with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "    with open(tokenizerFile) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "\n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    word_index = tokenizer_obj.word_index\n",
    "\n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "\n",
    "\n",
    "    num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        #embedding_vector = embeddings_index.get(word)\n",
    "        embedding_vector = w2v_model.wv[word] if word in w2v_model.wv else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return lines_pad, embedding_matrix, num_words, tokenizerFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e99615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWord2Vec(tokenized_list, tokenizer_path, max_length):\n",
    "    \n",
    "    with open(tokenizer_path) as f:\n",
    "        dataTokenizer = json.load(f)\n",
    "        tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "        \n",
    "    sequences = tokenizer_obj.texts_to_sequences(tokenized_list)\n",
    "    \n",
    "    lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "    \n",
    "    return lines_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "275db8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDLCrossValW2v_multi(X, y, max_len, dim, seed, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train, embedding_matrix, num_words, tokenizerFile = trainWord2Vec(X_train, max_len, dim)\n",
    "        \n",
    "        X_test = testWord2Vec(X_test, tokenizerFile, max_len)\n",
    "        \n",
    "#         Y_train = np.array(Y_train)\n",
    "#         Y_train = Y_train.ravel()\n",
    "#         Y_test = np.array(Y_test)\n",
    "#         Y_test = Y_test.ravel()\n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, multi, n_categories)\n",
    "        print(\"model summary\\m\",myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        \n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = np.argmax(predScores, axis=1)\n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions, average='micro')\n",
    "        recall=recall_score(Y_test, predictions, average='micro')\n",
    "        f1=f1_score(Y_test, predictions, average='micro')\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "        print(conf_matrix)\n",
    "        print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        del myModel\n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de802e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_w2v == True and multi == True and representation_form == \"sequences\":\n",
    "    dim = 300\n",
    "    runDLCrossValW2v_multi(np.array(tokenized_list), data[\"Category_Index\"].values, 512, dim, seed, \"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4504cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
